{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we built and trained a simple model to classify ASL images. The model was able to learn how to correctly classify the training dataset with very high accuracy, but, it did not perform nearly as well on validation dataset. This behavior of not generalizing well to non-training data is called [overfitting](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html), and in this section, we will introduce a popular kind of model called a [convolutional neural network](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53) that is especially good for reading images and classifying them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Prep data specifically for a CNN\n",
    "* Create a more sophisticated CNN model, understanding a greater variety of model layers\n",
    "* Train a CNN model and observe its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell contains the data preprocessing techniques we learned in the previous labs. Review it and execute it before moving on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 08:12:28.701607: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "\n",
    "# Load in our data from CSV files\n",
    "train_df = pd.read_csv(\"data/sign_mnist_train.csv\")\n",
    "valid_df = pd.read_csv(\"data/sign_mnist_valid.csv\")\n",
    "\n",
    "# Separate out our target values\n",
    "y_train = train_df['label']\n",
    "y_valid = valid_df['label']\n",
    "del train_df['label']\n",
    "del valid_df['label']\n",
    "\n",
    "# Separate out our image vectors\n",
    "x_train = train_df.values\n",
    "x_valid = valid_df.values\n",
    "\n",
    "# Turn our scalar targets into binary categories\n",
    "num_classes = 24\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_valid = keras.utils.to_categorical(y_valid, num_classes)\n",
    "\n",
    "# Normalize our image data\n",
    "x_train = x_train / 255\n",
    "x_valid = x_valid / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc523329430>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(x_train[0].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input must be >= 2-d.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/wendywang/Desktop/S1_2023_2024/Neural_Network_and_Deep_Learning/Lab3/Lab03_04_asl_cnn.ipynb Cell 9\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/wendywang/Desktop/S1_2023_2024/Neural_Network_and_Deep_Learning/Lab3/Lab03_04_asl_cnn.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m x_train_t \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mfliplr(x_train[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wendywang/Desktop/S1_2023_2024/Neural_Network_and_Deep_Learning/Lab3/Lab03_04_asl_cnn.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(x_train_t\u001b[39m.\u001b[39mreshape(\u001b[39m28\u001b[39m,\u001b[39m28\u001b[39m))\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mfliplr\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/twodim_base.py:97\u001b[0m, in \u001b[0;36mfliplr\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m     95\u001b[0m m \u001b[39m=\u001b[39m asanyarray(m)\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mndim \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInput must be >= 2-d.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     98\u001b[0m \u001b[39mreturn\u001b[39;00m m[:, ::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Input must be >= 2-d."
     ]
    }
   ],
   "source": [
    "x_train_t = np.fliplr(x_train[0])\n",
    "plt.imshow(x_train_t.reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pixel_values_list = []\n",
    "\n",
    "for i in range(1,22):\n",
    "    path = 'data/my_data/' + str(i) + '.png'\n",
    "    # print(path)\n",
    "    # 1. 加载图像\n",
    "    image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)  # 替换为你的图像文件路径\n",
    "    \n",
    "    # 2. 调整图像大小为28x28像素\n",
    "    image_resize = cv2.resize(image, (28, 28))\n",
    "    plt.imshow(image_resize)\n",
    "    # 4. 将像素值映射到0到255之间\n",
    "    pixel_values = (image_resize / 255)\n",
    "    pixel_values_list.append(pixel_values)\n",
    "\n",
    "pixel_values_list = np.asarray(pixel_values_list)\n",
    "\n",
    "print(pixel_values_list.shape)\n",
    "    # print(pixel_values)\n",
    "\n",
    "\n",
    "# # 3. 将图像转换为一维数组\n",
    "# # pixel_values = image.flatten()\n",
    "\n",
    "# # 4. 将像素值映射到0到255之间\n",
    "# pixel_values = (image / 255)\n",
    "# print(pixel_values.shape)\n",
    "# print(pixel_values)\n",
    "# # 5. 将像素值以空格分隔并打印出来，以符合MNIST数据集的格式\n",
    "# formatted_pixel_values = ' '.join(map(str, pixel_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping Images for a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last exercise, the individual pictures in our dataset are in the format of long lists of 784 pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27455, 784), (7172, 784))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this format, we don't have all the information about which pixels are near each other. Because of this, we can't apply convolutions that will detect features. Let's reshape our dataset so that they are in a 28x28 pixel format. This will allow our convolutions to associate groups of pixels and detect important features.\n",
    "\n",
    "Note that for the first convolutional layer of our model, we need to have not only the height and width of the image, but also the number of [color channels](https://www.photoshopessentials.com/essentials/rgb/). Our images are grayscale, so we'll just have 1 channel.\n",
    "\n",
    "That means that we need to convert the current shape `(27455, 784)` to `(27455, 28, 28, 1)`. As a convenience, we can pass the [reshape](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html#numpy.reshape) method a `-1` for any dimension we wish to remain the same, therefore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1,28,28,1)\n",
    "x_valid = x_valid.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27455, 28, 28, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7172, 28, 28, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27455, 28, 28, 1), (7172, 28, 28, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Convolutional Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These days, many data scientists start their projects by borrowing model properties from a similar project. Assuming the problem is not totally unique, there's a great chance that people have created models that will perform well which are posted in online repositories like [TensorFlow Hub](https://www.tensorflow.org/hub) and the [NGC Catalog](https://ngc.nvidia.com/catalog/models). Today, we'll provide a model that will work well for this problem.\n",
    "\n",
    "<img src=\"images/cnn.png\" width=180 />\n",
    "\n",
    "We covered many of the different kinds of layers in the lecture, and we will go over them all here with links to their documentation. When in doubt, read the official documentation (or ask [stackoverflow](https://stackoverflow.com/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Conv2D,\n",
    "    MaxPool2D,\n",
    "    Flatten,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(75, (3, 3), strides=1, padding=\"same\", activation=\"relu\", \n",
    "                 input_shape=(28, 28, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "model.add(Conv2D(50, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=256, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "model.add(Conv2D(25, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=num_classes, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/conv2d.png\" width=300 />\n",
    "\n",
    "These are our 2D convolutional layers. Small kernels will go over the input image and detect features that are important for classification. Earlier convolutions in the model will detect simple features such as lines. Later convolutions will detect more complex features. Let's look at our first Conv2D layer:\n",
    "```Python\n",
    "model.add(Conv2D(75 , (3,3) , strides = 1 , padding = 'same'...)\n",
    "```\n",
    "75 refers to the number of filters that will be learned. (3,3) refers to the size of those filters. Strides refer to the step size that the filter will take as it passes over the image. Padding refers to whether the output image that's created from the filter will match the size of the input image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [BatchNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like normalizing our inputs, batch normalization scales the values in the hidden layers to improve training. [Read more about it in detail here](https://blog.paperspace.com/busting-the-myths-about-batch-normalization/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/maxpool2d.png\" width=300 />\n",
    "Max pooling takes an image and essentially shrinks it to a lower resolution. It does this to help the model be robust to translation (objects moving side to side), and also makes our model faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dropout.png\" width=360 />\n",
    "Dropout is a technique for preventing overfitting. Dropout randomly selects a subset of neurons and turns them off, so that they do not participate in forward or backward propagation in that particular pass. This helps to make sure that the network is robust and redundant, and does not rely on any one area to come up with answers.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten takes the output of one layer which is multidimensional, and flattens it into a one-dimensional array. The output is called a feature vector and will be connected to the final classification layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen dense layers before in our earlier models. Our first dense layer (512 units) takes the feature vector as input and learns which features will contribute to a particular classification. The second dense layer (24 units) is the final classification layer that outputs our prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may feel like a lot of information, but don't worry. It's not critical that to understand everything right now in order to effectively train convolutional models. Most importantly we know that they can help with extracting useful information from images, and can be used in classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we summarize the model we just created. Notice how it has fewer trainable parameters than the model in the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 75)        750       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 28, 28, 75)       300       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 75)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 50)        33800     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 14, 50)        0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 14, 14, 256)       13056     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 14, 14, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 25)          57625     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 7, 7, 25)         100       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 25)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 400)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               205312    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 24)                12312     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 324,279\n",
      "Trainable params: 323,567\n",
      "Non-trainable params: 712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll compile the model just like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the very different model architecture, the training looks exactly the same. Run the cell below to train for 20 epochs and let's see if the accuracy improves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "858/858 [==============================] - 72s 83ms/step - loss: 0.3475 - accuracy: 0.8916 - val_loss: 0.2311 - val_accuracy: 0.9371\n",
      "Epoch 2/20\n",
      "858/858 [==============================] - 77s 90ms/step - loss: 0.0221 - accuracy: 0.9925 - val_loss: 0.6016 - val_accuracy: 0.8611\n",
      "Epoch 3/20\n",
      "858/858 [==============================] - 78s 91ms/step - loss: 0.0092 - accuracy: 0.9969 - val_loss: 0.1982 - val_accuracy: 0.9633\n",
      "Epoch 4/20\n",
      "858/858 [==============================] - 79s 92ms/step - loss: 0.0070 - accuracy: 0.9980 - val_loss: 1.0274 - val_accuracy: 0.8150\n",
      "Epoch 5/20\n",
      "858/858 [==============================] - 75s 88ms/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.3084 - val_accuracy: 0.9315\n",
      "Epoch 6/20\n",
      "858/858 [==============================] - 70s 81ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.3261 - val_accuracy: 0.9374\n",
      "Epoch 7/20\n",
      "858/858 [==============================] - 71s 83ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.2163 - val_accuracy: 0.9558\n",
      "Epoch 8/20\n",
      "858/858 [==============================] - 71s 82ms/step - loss: 6.7023e-04 - accuracy: 0.9998 - val_loss: 0.2431 - val_accuracy: 0.9679\n",
      "Epoch 9/20\n",
      "858/858 [==============================] - 71s 82ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.4253 - val_accuracy: 0.9452\n",
      "Epoch 10/20\n",
      "858/858 [==============================] - 71s 83ms/step - loss: 4.5982e-04 - accuracy: 0.9998 - val_loss: 0.2601 - val_accuracy: 0.9547\n",
      "Epoch 11/20\n",
      "858/858 [==============================] - 71s 83ms/step - loss: 8.5562e-05 - accuracy: 1.0000 - val_loss: 0.2069 - val_accuracy: 0.9647\n",
      "Epoch 12/20\n",
      "858/858 [==============================] - 72s 84ms/step - loss: 2.2740e-04 - accuracy: 0.9999 - val_loss: 0.2913 - val_accuracy: 0.9467\n",
      "Epoch 13/20\n",
      "858/858 [==============================] - 73s 85ms/step - loss: 5.0943e-04 - accuracy: 0.9999 - val_loss: 0.4874 - val_accuracy: 0.9212\n",
      "Epoch 14/20\n",
      "858/858 [==============================] - 72s 83ms/step - loss: 7.8945e-05 - accuracy: 1.0000 - val_loss: 0.2336 - val_accuracy: 0.9689\n",
      "Epoch 15/20\n",
      "858/858 [==============================] - 72s 84ms/step - loss: 2.6911e-04 - accuracy: 0.9999 - val_loss: 0.2455 - val_accuracy: 0.9625\n",
      "Epoch 16/20\n",
      "858/858 [==============================] - 72s 84ms/step - loss: 6.6218e-05 - accuracy: 1.0000 - val_loss: 0.2057 - val_accuracy: 0.9721\n",
      "Epoch 17/20\n",
      "858/858 [==============================] - 73s 85ms/step - loss: 4.5897e-04 - accuracy: 0.9999 - val_loss: 0.6656 - val_accuracy: 0.9000\n",
      "Epoch 18/20\n",
      "858/858 [==============================] - 72s 84ms/step - loss: 1.1085e-04 - accuracy: 1.0000 - val_loss: 0.2463 - val_accuracy: 0.9579\n",
      "Epoch 19/20\n",
      "858/858 [==============================] - 73s 85ms/step - loss: 1.5730e-04 - accuracy: 1.0000 - val_loss: 0.1963 - val_accuracy: 0.9637\n",
      "Epoch 20/20\n",
      "858/858 [==============================] - 78s 91ms/step - loss: 6.3454e-06 - accuracy: 1.0000 - val_loss: 0.4573 - val_accuracy: 0.9253\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc524417190>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=20, verbose=1, validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "result = np.argmax(model.predict(x_train_t.reshape(-1,28,28,1)))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 55ms/step\n",
      "4\n",
      "7\n",
      "6\n",
      "7\n",
      "7\n",
      "10\n",
      "6\n",
      "13\n",
      "6\n",
      "14\n",
      "6\n",
      "15\n",
      "22\n",
      "11\n",
      "2\n",
      "11\n",
      "4\n",
      "7\n",
      "12\n",
      "7\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "from mxnet import nd\n",
    "pixel_values=pixel_values_list.reshape(-1,28,28,1)\n",
    "result = model.predict(pixel_values)\n",
    "for p in result:\n",
    "    print(np.argmax(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like this model is significantly improved! The training accuracy is very high, and the validation accuracy has improved as well. This is a great result, as all we had to do was swap in a new model.\n",
    "\n",
    "You may have noticed the validation accuracy jumping around. This is an indication that our model is still not generalizing perfectly. Fortunately, there's more that we can do. Let's talk about it in the next lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we utilized several new kinds of layers to implement a CNN, which performed better than the more simple model used in the last section. Hopefully the overall process of creating and training a model with prepared data is starting to become even more familiar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last several sections you have focused on the creation and training of models. In order to further improve performance, you will now turn your attention to *data augmentation*, a collection of techniques that will allow your models to train on more and better data than what you might have originally at your disposal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
