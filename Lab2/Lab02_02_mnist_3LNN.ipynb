{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with the MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will do the \"Hello World\" of deep learning: training a deep learning model to correctly classify hand-written digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Understand how deep learning can solve problems traditional programming methods cannot\n",
    "* Learn about the [MNSIT handwritten digits dataset](http://yann.lecun.com/exdb/mnist/)\n",
    "* Use the [Keras API](https://keras.io/) to load the MNIST dataset and prepare it for training\n",
    "* Create a simple neural network to perform image classification\n",
    "* Train the neural network using the prepped MNIST dataset\n",
    "* Observe the performance of the trained neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem: Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In traditional programming, the programmer is able to articulate rules and conditions in their code that their program can then use to act in the correct way. This approach continues to work exceptionally well for a huge variety of problems.\n",
    "\n",
    "Image classification, which asks a program to correctly classify an image it has never seen before into its correct class, is near impossible to solve with traditional programming techniques. How could a programmer possibly define the rules and conditions to correctly classify a huge variety of images, especially taking into account images that they have never seen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Solution: Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning excels at pattern recognition by trial and error. By training a deep neural network with sufficient data, and providing the network with feedback on its performance via training, the network can identify, though a huge amount of iteration, its own set of conditions by which it can act in the correct way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the history of deep learning, the accurate image classification of the [MNSIT dataset](http://yann.lecun.com/exdb/mnist/), a collection of 70,000 grayscale images of handwritten digits from 0 to 9, was a major development. While today the problem is considered trivial, doing image classification with MNIST has become a kind of \"Hello World\" for deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Data and Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with images for deep learning, we need both the images themselves, usually denoted as `X`, and also, correct [labels](https://developers.google.com/machine-learning/glossary#label) for these images, usually denoted as `Y`. Furthermore, we need `X` and `Y` values both for *training* the model, and then, a separate set of `X` and `Y` values for *validating* the performance of the model after it has been trained. Therefore, we need 4 segments of data for the MNIST dataset:\n",
    "\n",
    "1. `x_train`: Images used for training the neural network\n",
    "2. `y_train`: Correct labels for the `x_train` images, used to evaluate the model's predictions during training\n",
    "3. `x_valid`: Images set aside for validating the performance of the model after it has been trained\n",
    "4. `y_valid`: Correct labels for the `x_valid` images, used to evaluate the model's predictions after it has been trained\n",
    "\n",
    "The process of preparing data for analysis is called [Data Engineering](https://medium.com/@rchang/a-beginners-guide-to-data-engineering-part-i-4227c5c457d7). To learn more about the differences between training data and validation data (as well as test data), check out [this article](https://machinelearningmastery.com/difference-test-validation-datasets/) by Jason Brownlee."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data Into Memory (with Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many [deep learning frameworks](https://developer.nvidia.com/deep-learning-frameworks), each with their own merits. In this workshop we will be working with [Tensorflow 2](https://www.tensorflow.org/tutorials/quickstart/beginner), and specifically with the [Keras API](https://keras.io/). Keras has many useful built in functions designed for the computer vision tasks. It is also a legitimate choice for deep learning in a professional setting due to its [readability](https://blog.pragmaticengineer.com/readable-code/) and efficiency, though it is not alone in this regard, and it is worth investigating a variety of frameworks when beginning a deep learning project.\n",
    "\n",
    "One of the many helpful features that Keras provides are modules containing many helper methods for [many common datasets](https://www.tensorflow.org/api_docs/python/tf/keras/datasets), including MNIST.\n",
    "\n",
    "We will begin by loading the Keras dataset module for MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-07 21:00:19.170430: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `mnist` module, we can easily load the MNIST data, already partitioned into images and labels for both training and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, split between train and validation sets\n",
    "(x_train, y_train), (x_valid, y_valid) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the MNIST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stated above that the MNIST dataset contained 70,000 grayscale images of handwritten digits. By executing the following cells, we can see that Keras has partitioned 60,000 of these images for training, and 10,000 for validation (after training), and also, that each image itself is a 2D array with the dimensions 28x28:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we can see that these 28x28 images are represented as a collection of unsigned 8-bit integer values between 0 and 255, the values corresponding with a pixel's grayscale value where `0` is black, `255` is white, and all other values are in between:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [Matplotlib](https://matplotlib.org/), we can render one of these grayscale images in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f95960f40d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAN80lEQVR4nO3df6hcdXrH8c+ncf3DrBpTMYasNhuRWBWbLRqLSl2RrD9QNOqWDVgsBrN/GHChhEr6xyolEuqP0qAsuYu6sWyzLqgYZVkVo6ZFCF5j1JjU1YrdjV6SSozG+KtJnv5xT+Su3vnOzcyZOZP7vF9wmZnzzJnzcLife87Md879OiIEYPL7k6YbANAfhB1IgrADSRB2IAnCDiRxRD83ZpuP/oEeiwiPt7yrI7vtS22/aftt27d281oAesudjrPbniLpd5IWSNou6SVJiyJia2EdjuxAj/XiyD5f0tsR8U5EfCnpV5Ku6uL1APRQN2GfJekPYx5vr5b9EdtLbA/bHu5iWwC61M0HdOOdKnzjND0ihiQNSZzGA03q5si+XdJJYx5/R9L73bUDoFe6CftLkk61/V3bR0r6kaR19bQFoG4dn8ZHxD7bSyU9JWmKpAci4o3aOgNQq46H3jraGO/ZgZ7ryZdqABw+CDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUii4ymbcXiYMmVKsX7sscf2dPtLly5tWTvqqKOK686dO7dYv/nmm4v1u+66q2Vt0aJFxXU///zzYn3lypXF+u23316sN6GrsNt+V9IeSfsl7YuIs+toCkD96jiyXxQRH9TwOgB6iPfsQBLdhj0kPW37ZdtLxnuC7SW2h20Pd7ktAF3o9jT+/Ih43/YJkp6x/V8RsWHsEyJiSNKQJNmOLrcHoENdHdkj4v3qdqekxyTNr6MpAPXrOOy2p9o++uB9ST+QtKWuxgDUq5vT+BmSHrN98HX+PSJ+W0tXk8zJJ59crB955JHF+nnnnVesX3DBBS1r06ZNK6577bXXFutN2r59e7G+atWqYn3hwoUta3v27Cmu++qrrxbrL7zwQrE+iDoOe0S8I+kvauwFQA8x9AYkQdiBJAg7kARhB5Ig7EASjujfl9om6zfo5s2bV6yvX7++WO/1ZaaD6sCBA8X6jTfeWKx/8sknHW97ZGSkWP/www+L9TfffLPjbfdaRHi85RzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtlrMH369GJ948aNxfqcOXPqbKdW7XrfvXt3sX7RRRe1rH355ZfFdbN+/6BbjLMDyRF2IAnCDiRB2IEkCDuQBGEHkiDsQBJM2VyDXbt2FevLli0r1q+44opi/ZVXXinW2/1L5ZLNmzcX6wsWLCjW9+7dW6yfccYZLWu33HJLcV3UiyM7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTB9ewD4JhjjinW200vvHr16pa1xYsXF9e9/vrri/W1a9cW6xg8HV/PbvsB2zttbxmzbLrtZ2y/Vd0eV2ezAOo3kdP4X0i69GvLbpX0bEScKunZ6jGAAdY27BGxQdLXvw96laQ11f01kq6uty0Adev0u/EzImJEkiJixPYJrZ5oe4mkJR1uB0BNen4hTEQMSRqS+IAOaFKnQ287bM+UpOp2Z30tAeiFTsO+TtIN1f0bJD1eTzsAeqXtabzttZK+L+l429sl/VTSSkm/tr1Y0u8l/bCXTU52H3/8cVfrf/TRRx2ve9NNNxXrDz/8cLHebo51DI62YY+IRS1KF9fcC4Ae4uuyQBKEHUiCsANJEHYgCcIOJMElrpPA1KlTW9aeeOKJ4roXXnhhsX7ZZZcV608//XSxjv5jymYgOcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJx9knulFNOKdY3bdpUrO/evbtYf+6554r14eHhlrX77ruvuG4/fzcnE8bZgeQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtmTW7hwYbH+4IMPFutHH310x9tevnx5sf7QQw8V6yMjIx1vezJjnB1IjrADSRB2IAnCDiRB2IEkCDuQBGEHkmCcHUVnnnlmsX7PPfcU6xdf3Plkv6tXry7WV6xYUay/9957HW/7cNbxOLvtB2zvtL1lzLLbbL9ne3P1c3mdzQKo30RO438h6dJxlv9LRMyrfn5Tb1sA6tY27BGxQdKuPvQCoIe6+YBuqe3XqtP841o9yfYS28O2W/8zMgA912nYfybpFEnzJI1IurvVEyNiKCLOjoizO9wWgBp0FPaI2BER+yPigKSfS5pfb1sA6tZR2G3PHPNwoaQtrZ4LYDC0HWe3vVbS9yUdL2mHpJ9Wj+dJCknvSvpxRLS9uJhx9sln2rRpxfqVV17ZstbuWnl73OHir6xfv75YX7BgQbE+WbUaZz9iAisuGmfx/V13BKCv+LoskARhB5Ig7EAShB1IgrADSXCJKxrzxRdfFOtHHFEeLNq3b1+xfskll7SsPf/888V1D2f8K2kgOcIOJEHYgSQIO5AEYQeSIOxAEoQdSKLtVW/I7ayzzirWr7vuumL9nHPOaVlrN47eztatW4v1DRs2dPX6kw1HdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2SW7u3LnF+tKlS4v1a665plg/8cQTD7mnidq/f3+xPjJS/u/lBw4cqLOdwx5HdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2w0C7sexFi8abaHdUu3H02bNnd9JSLYaHh4v1FStWFOvr1q2rs51Jr+2R3fZJtp+zvc32G7ZvqZZPt/2M7beq2+N63y6ATk3kNH6fpL+PiD+X9FeSbrZ9uqRbJT0bEadKerZ6DGBAtQ17RIxExKbq/h5J2yTNknSVpDXV09ZIurpHPQKowSG9Z7c9W9L3JG2UNCMiRqTRPwi2T2ixzhJJS7rsE0CXJhx229+W9Iikn0TEx/a4c8d9Q0QMSRqqXoOJHYGGTGjozfa3NBr0X0bEo9XiHbZnVvWZknb2pkUAdWh7ZPfoIfx+Sdsi4p4xpXWSbpC0srp9vCcdTgIzZswo1k8//fRi/d577y3WTzvttEPuqS4bN24s1u+8886WtccfL//KcIlqvSZyGn++pL+V9LrtzdWy5RoN+a9tL5b0e0k/7EmHAGrRNuwR8Z+SWr1Bv7jedgD0Cl+XBZIg7EAShB1IgrADSRB2IAkucZ2g6dOnt6ytXr26uO68efOK9Tlz5nTSUi1efPHFYv3uu+8u1p966qli/bPPPjvkntAbHNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IIk04+znnntusb5s2bJiff78+S1rs2bN6qinunz66acta6tWrSque8cddxTre/fu7agnDB6O7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQRJpx9oULF3ZV78bWrVuL9SeffLJY37dvX7FeuuZ89+7dxXWRB0d2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUjCEVF+gn2SpIcknSjpgKShiPhX27dJuknS/1ZPXR4Rv2nzWuWNAehaRIw76/JEwj5T0syI2GT7aEkvS7pa0t9I+iQi7ppoE4Qd6L1WYZ/I/Owjkkaq+3tsb5PU7L9mAXDIDuk9u+3Zkr4naWO1aKnt12w/YPu4FusssT1se7i7VgF0o+1p/FdPtL8t6QVJKyLiUdszJH0gKST9k0ZP9W9s8xqcxgM91vF7dkmy/S1JT0p6KiLuGac+W9KTEXFmm9ch7ECPtQp729N425Z0v6RtY4NefXB30EJJW7ptEkDvTOTT+Ask/Yek1zU69CZJyyUtkjRPo6fx70r6cfVhXum1OLIDPdbVaXxdCDvQex2fxgOYHAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ9HvK5g8k/c+Yx8dXywbRoPY2qH1J9NapOnv7s1aFvl7P/o2N28MRcXZjDRQMam+D2pdEb53qV2+cxgNJEHYgiabDPtTw9ksGtbdB7Uuit071pbdG37MD6J+mj+wA+oSwA0k0Enbbl9p+0/bbtm9toodWbL9r+3Xbm5uen66aQ2+n7S1jlk23/Yztt6rbcefYa6i322y/V+27zbYvb6i3k2w/Z3ub7Tds31Itb3TfFfrqy37r+3t221Mk/U7SAknbJb0kaVFEbO1rIy3YflfS2RHR+BcwbP+1pE8kPXRwai3b/yxpV0SsrP5QHhcR/zAgvd2mQ5zGu0e9tZpm/O/U4L6rc/rzTjRxZJ8v6e2IeCcivpT0K0lXNdDHwIuIDZJ2fW3xVZLWVPfXaPSXpe9a9DYQImIkIjZV9/dIOjjNeKP7rtBXXzQR9lmS/jDm8XYN1nzvIelp2y/bXtJ0M+OYcXCarer2hIb7+bq203j309emGR+YfdfJ9OfdaiLs401NM0jjf+dHxF9KukzSzdXpKibmZ5JO0egcgCOS7m6ymWqa8Uck/SQiPm6yl7HG6asv+62JsG+XdNKYx9+R9H4DfYwrIt6vbndKekyjbzsGyY6DM+hWtzsb7ucrEbEjIvZHxAFJP1eD+66aZvwRSb+MiEerxY3vu/H66td+ayLsL0k61fZ3bR8p6UeS1jXQxzfYnlp9cCLbUyX9QIM3FfU6STdU92+Q9HiDvfyRQZnGu9U042p43zU+/XlE9P1H0uUa/UT+vyX9YxM9tOhrjqRXq583mu5N0lqNntb9n0bPiBZL+lNJz0p6q7qdPkC9/ZtGp/Z+TaPBmtlQbxdo9K3ha5I2Vz+XN73vCn31Zb/xdVkgCb5BByRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ/D+f1mbt6t55/AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = x_train[0]\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way we can now see that this is a 28x28 pixel image of a 5. Or is it a 3? The answer is in the `y_train` data, which contains correct labels for the data. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning, it is common that data needs to be transformed to be in the ideal state for training. For this particular image classification problem, there are 3 tasks we should perform with the data in preparation for training:\n",
    "1. Flatten the image data, to simplify the image input into the model\n",
    "2. Normalize the image data, to make the image input values easier to work with for the model\n",
    "3. Categorize the labels, to make the label values easier to work with for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening the Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though it's possible for a deep learning model to accept a 2-dimensional image (in our case 28x28 pixels), we're going to simplify things to start and [reshape](https://www.tensorflow.org/api_docs/python/tf/reshape) each image into a single array of 784 continuous pixels (note: 28x28 = 784). This is also called flattening the image.\n",
    "\n",
    "Here we accomplish this using the helper method `reshape`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000, 784)\n",
    "x_valid = x_valid.reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that the image data has been reshaped and is now a collection of 1D arrays containing 784 pixel values each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
       "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
       "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
       "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
       "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
       "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
       "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
       "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
       "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning models are better at dealing with floating point numbers between 0 and 1 (more on this topic later). Converting integer values to floating point values between 0 and 1 is called [normalization](https://developers.google.com/machine-learning/glossary#normalization), and a simple approach we will take here to normalize the data will be to divide all the pixel values (which if you recall are between 0 and 255) by 255:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255\n",
    "x_valid = x_valid / 255 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that the values are all floating point values between `0.0` and `1.0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider for a moment, if we were to ask, what is 7 - 2? Stating that the answer was 4 is closer than stating that the answer was 9. However, for this image classification problem, we don't want the neural network to learn this kind of reasoning: we just want it to select the correct category, and understand that if we have an image of the number 5, that guessing 4 is just as bad as guessing 9.\n",
    "\n",
    "As it stands, the labels for the images are integers between 0 and 9. Because these values represent a numerical range, the model might try to draw some conclusions about its performance based on how close to the correct numerical category it guesses.\n",
    "\n",
    "Therefore, we will do something to our data called categorical encoding. This kind of transformation modifies the data so that each value is a collection of all possible categories, with the actual category that this particular value is set as true.\n",
    "\n",
    "As a simple example, consider if we had 3 categories: red, blue, and green. For a given color, 2 of these categories would be false, and the other would be true:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Actual Color| Is Red? | Is Blue? | Is Green?|\n",
    "|------------|---------|----------|----------|\n",
    "|Red|True|False|False|\n",
    "|Green|False|False|True|\n",
    "|Blue|False|True|False|\n",
    "|Green|False|False|True|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than use \"True\" or \"False\", we could represent the same using binary, either 0 or 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Actual Color| Is Red? | Is Blue? | Is Green?|\n",
    "|------------|---------|----------|----------|\n",
    "|Red|1|0|0|\n",
    "|Green|0|0|1|\n",
    "|Blue|0|1|0|\n",
    "|Green|0|0|1|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what categorical encoding is, transforming values which are intended to be understood as categorical labels into a representation that makes their categorical nature explicit to the model. Thus, if we were using these values for training, we would convert..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "values = ['red, green, blue, green']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... which a neural network would have a very difficult time making sense of, instead to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "values = [\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorically Encoding the Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides a utility to [categorically encode values](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical), and here we use it to perform categorical encoding for both the training and validation labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "num_categories = 10\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_categories)\n",
    "y_valid = keras.utils.to_categorical(y_valid, num_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first 10 values of the training labels, which you can see have now been categorically encoded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data prepared for training, it is now time to create the model that we will train with the data. This first basic model will be made up of several *layers* and will be comprised of 3 main parts:\n",
    "\n",
    "1. An input layer, which will receive data in some expected format\n",
    "2. Several [hidden layers](https://developers.google.com/machine-learning/glossary#hidden-layer), each comprised of many *neurons*. Each [neuron](https://developers.google.com/machine-learning/glossary#neuron) will have the ability to affect the network's guess with its *weights*, which are values that will be updated over many iterations as the network gets feedback on its performance and learns\n",
    "3. An output layer, which will depict the network's guess for a given image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we will use Keras's [Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) model class to instantiate an instance of a model that will have a series of layers that data will pass through in sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Input Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add the input layer. This layer will be *densely connected*, meaning that each neuron in it, and its weights, will affect every neuron in the next layer. To do this with Keras, we use Keras's [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `units` argument specifies the number of neurons in the layer. We are going to use `512` which we have chosen from experimentation. Choosing the correct number of neurons is what puts the \"science\" in \"data science\" as it is a matter of capturing the statistical complexity of the dataset. Try playing around with this value later to see how it affects training and to start developing a sense for what this number means.\n",
    "\n",
    "We will learn more about activation functions later, but for now, we will use the `relu` activation function, which in short, will help our network to learn how to make more sophisticated guesses about data than if it were required to make guesses based on some strictly linear function.\n",
    "\n",
    "The `input_shape` value specifies the shape of the incoming data which in our situation is a 1D array of 784 values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=512, activation='relu', input_shape=(784,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will add an additional densely connected layer. Again, much more will be said about these later, but for now know that these layers give the network more parameters to contribute towards its guesses, and therefore, more subtle opportunities for accurate learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units = 512, activation='relu'))\n",
    "# model.add(Dense(units = 256, activation='relu'))\n",
    "# model.add(Dense(units = 256, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will add an output layer. This layer uses the activation function `softmax` which will result in each of the layer's values being a probability between 0 and 1 and will result in all the outputs of the layer adding to 1. In this case, since the network is to make a guess about a single image belonging to 1 of 10 possible categories, there will be 10 outputs. Each output gives the model's guess (a probability) that the image belongs to that specific class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units = 10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides the model instance method [summary](https://www.tensorflow.org/api_docs/python/tf/summary) which will print a readable summary of a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 512)               401920    \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the number of trainable parameters. Each of these can be adjusted during training and will contribute towards the trained model's guesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, more details are to follow, but the final step we need to do before we can actually train our model with data is to [compile](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#compile) it. Here we specify a [loss function](https://developers.google.com/machine-learning/glossary#loss) which will be used for the model to understand how well it is performing during training. We also specify that we would like to track `accuracy` while the model trains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model.compile(optimizer = tf.keras.optimizers.SGD(learning_rate = 0.01), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have prepared training and validation data, and a model, it's time to train our model with our training data, and verify it with its validation data.\n",
    "\n",
    "\"Training a model with data\" is often also called \"fitting a model to data.\" Put this latter way, it highlights that the shape of the model changes over time to more accurately understand the data that it is being given.\n",
    "\n",
    "When fitting (training) a model with Keras, we use the model's [fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) method. It expects the following arguments:\n",
    "\n",
    "* The training data\n",
    "* The labels for the training data\n",
    "* The number of times it should train on the entire training dataset (called an *epoch*)\n",
    "* The validation or test data, and its labels\n",
    "\n",
    "Run the cell below to train the model. We will discuss its output after the training completes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1446 - accuracy: 0.9588 - val_loss: 0.1478 - val_accuracy: 0.9574\n",
      "Epoch 2/50\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1444 - accuracy: 0.9591 - val_loss: 0.1476 - val_accuracy: 0.9573\n",
      "Epoch 3/50\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1442 - accuracy: 0.9592 - val_loss: 0.1474 - val_accuracy: 0.9573\n",
      "Epoch 4/50\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1439 - accuracy: 0.9592 - val_loss: 0.1472 - val_accuracy: 0.9572\n",
      "Epoch 5/50\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1437 - accuracy: 0.9594 - val_loss: 0.1470 - val_accuracy: 0.9573\n",
      "Epoch 6/50\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1435 - accuracy: 0.9594 - val_loss: 0.1470 - val_accuracy: 0.9574\n",
      "Epoch 7/50\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1433 - accuracy: 0.9597 - val_loss: 0.1467 - val_accuracy: 0.9573\n",
      "Epoch 8/50\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1431 - accuracy: 0.9596 - val_loss: 0.1466 - val_accuracy: 0.9574\n",
      "Epoch 9/50\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1429 - accuracy: 0.9598 - val_loss: 0.1464 - val_accuracy: 0.9575\n",
      "Epoch 10/50\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1427 - accuracy: 0.9598 - val_loss: 0.1463 - val_accuracy: 0.9575\n",
      "Epoch 11/50\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1425 - accuracy: 0.9600 - val_loss: 0.1461 - val_accuracy: 0.9573\n",
      "Epoch 12/50\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1423 - accuracy: 0.9599 - val_loss: 0.1460 - val_accuracy: 0.9574\n",
      "Epoch 13/50\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1421 - accuracy: 0.9601 - val_loss: 0.1459 - val_accuracy: 0.9575\n",
      "Epoch 14/50\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1419 - accuracy: 0.9602 - val_loss: 0.1457 - val_accuracy: 0.9574\n",
      "Epoch 15/50\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1417 - accuracy: 0.9603 - val_loss: 0.1456 - val_accuracy: 0.9573\n",
      "Epoch 16/50\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.1415 - accuracy: 0.9604 - val_loss: 0.1454 - val_accuracy: 0.9575\n",
      "Epoch 17/50\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1413 - accuracy: 0.9604 - val_loss: 0.1452 - val_accuracy: 0.9575\n",
      "Epoch 18/50\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1411 - accuracy: 0.9604 - val_loss: 0.1451 - val_accuracy: 0.9574\n",
      "Epoch 19/50\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1409 - accuracy: 0.9606 - val_loss: 0.1449 - val_accuracy: 0.9575\n",
      "Epoch 20/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1408 - accuracy: 0.9607 - val_loss: 0.1448 - val_accuracy: 0.9575\n",
      "Epoch 21/50\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1406 - accuracy: 0.9607 - val_loss: 0.1446 - val_accuracy: 0.9575\n",
      "Epoch 22/50\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1404 - accuracy: 0.9607 - val_loss: 0.1445 - val_accuracy: 0.9576\n",
      "Epoch 23/50\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1402 - accuracy: 0.9608 - val_loss: 0.1443 - val_accuracy: 0.9577\n",
      "Epoch 24/50\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1400 - accuracy: 0.9608 - val_loss: 0.1442 - val_accuracy: 0.9577\n",
      "Epoch 25/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1398 - accuracy: 0.9609 - val_loss: 0.1440 - val_accuracy: 0.9577\n",
      "Epoch 26/50\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1397 - accuracy: 0.9610 - val_loss: 0.1439 - val_accuracy: 0.9579\n",
      "Epoch 27/50\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1395 - accuracy: 0.9611 - val_loss: 0.1437 - val_accuracy: 0.9579\n",
      "Epoch 28/50\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1393 - accuracy: 0.9611 - val_loss: 0.1436 - val_accuracy: 0.9580\n",
      "Epoch 29/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1391 - accuracy: 0.9610 - val_loss: 0.1435 - val_accuracy: 0.9581\n",
      "Epoch 30/50\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.1390 - accuracy: 0.9612 - val_loss: 0.1433 - val_accuracy: 0.9579\n",
      "Epoch 31/50\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1388 - accuracy: 0.9612 - val_loss: 0.1432 - val_accuracy: 0.9581\n",
      "Epoch 32/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1386 - accuracy: 0.9613 - val_loss: 0.1431 - val_accuracy: 0.9580\n",
      "Epoch 33/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1384 - accuracy: 0.9614 - val_loss: 0.1429 - val_accuracy: 0.9581\n",
      "Epoch 34/50\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1383 - accuracy: 0.9613 - val_loss: 0.1427 - val_accuracy: 0.9580\n",
      "Epoch 35/50\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1381 - accuracy: 0.9615 - val_loss: 0.1426 - val_accuracy: 0.9582\n",
      "Epoch 36/50\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1379 - accuracy: 0.9616 - val_loss: 0.1425 - val_accuracy: 0.9582\n",
      "Epoch 37/50\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1377 - accuracy: 0.9616 - val_loss: 0.1423 - val_accuracy: 0.9582\n",
      "Epoch 38/50\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1376 - accuracy: 0.9616 - val_loss: 0.1421 - val_accuracy: 0.9581\n",
      "Epoch 39/50\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1374 - accuracy: 0.9617 - val_loss: 0.1419 - val_accuracy: 0.9582\n",
      "Epoch 40/50\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1372 - accuracy: 0.9618 - val_loss: 0.1418 - val_accuracy: 0.9583\n",
      "Epoch 41/50\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1371 - accuracy: 0.9617 - val_loss: 0.1417 - val_accuracy: 0.9584\n",
      "Epoch 42/50\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1369 - accuracy: 0.9618 - val_loss: 0.1416 - val_accuracy: 0.9584\n",
      "Epoch 43/50\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1367 - accuracy: 0.9619 - val_loss: 0.1414 - val_accuracy: 0.9585\n",
      "Epoch 44/50\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1365 - accuracy: 0.9618 - val_loss: 0.1413 - val_accuracy: 0.9586\n",
      "Epoch 45/50\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1364 - accuracy: 0.9621 - val_loss: 0.1411 - val_accuracy: 0.9585\n",
      "Epoch 46/50\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1362 - accuracy: 0.9621 - val_loss: 0.1410 - val_accuracy: 0.9587\n",
      "Epoch 47/50\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1360 - accuracy: 0.9620 - val_loss: 0.1409 - val_accuracy: 0.9587\n",
      "Epoch 48/50\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1359 - accuracy: 0.9621 - val_loss: 0.1407 - val_accuracy: 0.9587\n",
      "Epoch 49/50\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1357 - accuracy: 0.9622 - val_loss: 0.1406 - val_accuracy: 0.9588\n",
      "Epoch 50/50\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1355 - accuracy: 0.9622 - val_loss: 0.1404 - val_accuracy: 0.9589\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train, epochs=5, verbose=1, validation_data=(x_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f9544f80df0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(history.history.keys())\n",
    "epochs=5\n",
    "\n",
    "val_acc = history.history['val_accuracy']\n",
    "val_loss = history.history['val_loss']\n",
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "\n",
    "plt.plot(range(epochs), acc, 'b*-', label = 'Training accuracy')\n",
    "plt.plot(range(epochs), val_acc, 'r', label = 'Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f9544fc8d90>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(epochs), loss, 'b*-', label = 'Training loss')\n",
    "plt.plot(range(epochs), val_loss, 'r', label = 'Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the 5 epochs, notice the `accuracy` and `val_accuracy` scores. `accuracy` states how well the model did for the epoch on all the training data. `val_accuracy` states how well the model did on the validation data, which if you recall, was not used at all for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model did quite well! The accuracy quickly reached close to 100%, as did the validation accuracy. We now have a model that can be used to accurately detect and classify hand-written images.\n",
    "\n",
    "The next step would be to use this model to classify new not-yet-seen handwritten images. This is called [inference](https://blogs.nvidia.com/blog/2016/08/22/difference-deep-learning-training-inference-ai/). We'll explore the process of inference in a later exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth taking a moment to appreciate what we've done here. Historically, the expert systems that were built to do this kind of task were extremely complicated, and people spent their careers building them (check out the references on the [official MNIST page](http://yann.lecun.com/exdb/mnist/) and the years milestones were reached).\n",
    "\n",
    "MNIST is not only useful for its historical influence on Computer Vision, but it's also a great [benchmark](http://www.cs.toronto.edu/~serailhydra/publications/tbd-iiswc18.pdf) and debugging tool. Having trouble getting a fancy new machine learning architecture working? Check it against MNIST. If it can't learn on this dataset, chances are it won't learn on more complicated images and datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bonus Exercise \n",
    "\n",
    "Have time to spare? In the next section, we will talk about how we arrived at some of the numbers above, but we can try imagining what it was like to be a researcher developing the techniques commonly used today.\n",
    "\n",
    "Ultimately, each neuron is trying to fit a line to some data. Below, we have some datapoints and a randomly drawn line using the equation [y = mx + b](https://www.mathsisfun.com/equation_of_line.html).\n",
    "\n",
    "Try changing the `m` and the `b` in order to find the lowest possible loss. How did you find the best line? Can you make a program to follow your strategy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 475.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "m = -2  # -2 to start, change me please\n",
    "b = 40  # 40 to start, change me please\n",
    "\n",
    "# Sample data\n",
    "x = np.array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9])\n",
    "y = np.array([10, 20, 25, 30, 40, 45, 40, 50, 60, 55])\n",
    "y_hat = x * m + b\n",
    "\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, y_hat, '-')\n",
    "plt.show()\n",
    "\n",
    "print(\"Loss:\", np.sum((y - y_hat)**2)/len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have an idea? Excellent! Please shut down the kernel before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "model_temp = Sequential()\n",
    "model_temp.add(Dense(units=64, activation='relu', input_shape=(1,)))\n",
    "model_temp.add(Dense(units=32, activation='relu', input_shape=(1,)))\n",
    "model_temp.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 64)                128       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,241\n",
      "Trainable params: 2,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_temp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 813ms/step - loss: 1548.6893 - accuracy: 0.0000e+00\n",
      "Epoch 2/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1542.1570 - accuracy: 0.0000e+00\n",
      "Epoch 3/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1535.7307 - accuracy: 0.0000e+00\n",
      "Epoch 4/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1529.3024 - accuracy: 0.0000e+00\n",
      "Epoch 5/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1522.8665 - accuracy: 0.0000e+00\n",
      "Epoch 6/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1516.4104 - accuracy: 0.0000e+00\n",
      "Epoch 7/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1509.9336 - accuracy: 0.0000e+00\n",
      "Epoch 8/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1503.4360 - accuracy: 0.0000e+00\n",
      "Epoch 9/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1496.9163 - accuracy: 0.0000e+00\n",
      "Epoch 10/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1490.5244 - accuracy: 0.0000e+00\n",
      "Epoch 11/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1484.1143 - accuracy: 0.0000e+00\n",
      "Epoch 12/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1477.6814 - accuracy: 0.0000e+00\n",
      "Epoch 13/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1471.3489 - accuracy: 0.0000e+00\n",
      "Epoch 14/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1465.1562 - accuracy: 0.0000e+00\n",
      "Epoch 15/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1459.0276 - accuracy: 0.0000e+00\n",
      "Epoch 16/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1452.8464 - accuracy: 0.0000e+00\n",
      "Epoch 17/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1446.5155 - accuracy: 0.0000e+00\n",
      "Epoch 18/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1439.6793 - accuracy: 0.0000e+00\n",
      "Epoch 19/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1432.6101 - accuracy: 0.0000e+00\n",
      "Epoch 20/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1425.3357 - accuracy: 0.0000e+00\n",
      "Epoch 21/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1417.7783 - accuracy: 0.0000e+00\n",
      "Epoch 22/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1409.5833 - accuracy: 0.0000e+00\n",
      "Epoch 23/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1401.0892 - accuracy: 0.0000e+00\n",
      "Epoch 24/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1392.3395 - accuracy: 0.0000e+00\n",
      "Epoch 25/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1383.2952 - accuracy: 0.0000e+00\n",
      "Epoch 26/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1374.0177 - accuracy: 0.0000e+00\n",
      "Epoch 27/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1364.5677 - accuracy: 0.0000e+00\n",
      "Epoch 28/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1354.9866 - accuracy: 0.0000e+00\n",
      "Epoch 29/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1345.5446 - accuracy: 0.0000e+00\n",
      "Epoch 30/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1336.6063 - accuracy: 0.0000e+00\n",
      "Epoch 31/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1327.5199 - accuracy: 0.0000e+00\n",
      "Epoch 32/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1318.2910 - accuracy: 0.0000e+00\n",
      "Epoch 33/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1308.8904 - accuracy: 0.0000e+00\n",
      "Epoch 34/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1299.1991 - accuracy: 0.0000e+00\n",
      "Epoch 35/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1289.0022 - accuracy: 0.0000e+00\n",
      "Epoch 36/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1278.5612 - accuracy: 0.0000e+00\n",
      "Epoch 37/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1267.9172 - accuracy: 0.0000e+00\n",
      "Epoch 38/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1257.0895 - accuracy: 0.0000e+00\n",
      "Epoch 39/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1246.0928 - accuracy: 0.0000e+00\n",
      "Epoch 40/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1234.9393 - accuracy: 0.0000e+00\n",
      "Epoch 41/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1223.6377 - accuracy: 0.0000e+00\n",
      "Epoch 42/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1212.1952 - accuracy: 0.0000e+00\n",
      "Epoch 43/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1200.6182 - accuracy: 0.0000e+00\n",
      "Epoch 44/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1188.9111 - accuracy: 0.0000e+00\n",
      "Epoch 45/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1177.0784 - accuracy: 0.0000e+00\n",
      "Epoch 46/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1165.1234 - accuracy: 0.0000e+00\n",
      "Epoch 47/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1153.0486 - accuracy: 0.0000e+00\n",
      "Epoch 48/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1140.8571 - accuracy: 0.0000e+00\n",
      "Epoch 49/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1128.5496 - accuracy: 0.0000e+00\n",
      "Epoch 50/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1116.1287 - accuracy: 0.0000e+00\n",
      "Epoch 51/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1103.5959 - accuracy: 0.0000e+00\n",
      "Epoch 52/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1090.9526 - accuracy: 0.0000e+00\n",
      "Epoch 53/1000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1078.2006 - accuracy: 0.0000e+00\n",
      "Epoch 54/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1065.3414 - accuracy: 0.0000e+00\n",
      "Epoch 55/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1052.3762 - accuracy: 0.0000e+00\n",
      "Epoch 56/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1039.3064 - accuracy: 0.0000e+00\n",
      "Epoch 57/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1026.1339 - accuracy: 0.0000e+00\n",
      "Epoch 58/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1012.8600 - accuracy: 0.0000e+00\n",
      "Epoch 59/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 999.4869 - accuracy: 0.0000e+00\n",
      "Epoch 60/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 986.0161 - accuracy: 0.0000e+00\n",
      "Epoch 61/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 972.4502 - accuracy: 0.0000e+00\n",
      "Epoch 62/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 958.7909 - accuracy: 0.0000e+00\n",
      "Epoch 63/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 945.0403 - accuracy: 0.0000e+00\n",
      "Epoch 64/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 931.2010 - accuracy: 0.0000e+00\n",
      "Epoch 65/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 917.2760 - accuracy: 0.0000e+00\n",
      "Epoch 66/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 903.2678 - accuracy: 0.0000e+00\n",
      "Epoch 67/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 889.1799 - accuracy: 0.0000e+00\n",
      "Epoch 68/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 875.0154 - accuracy: 0.0000e+00\n",
      "Epoch 69/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 860.7778 - accuracy: 0.0000e+00\n",
      "Epoch 70/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 846.4719 - accuracy: 0.0000e+00\n",
      "Epoch 71/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 832.1008 - accuracy: 0.0000e+00\n",
      "Epoch 72/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 817.6689 - accuracy: 0.0000e+00\n",
      "Epoch 73/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 803.1808 - accuracy: 0.0000e+00\n",
      "Epoch 74/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 788.6411 - accuracy: 0.0000e+00\n",
      "Epoch 75/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 774.0551 - accuracy: 0.0000e+00\n",
      "Epoch 76/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 759.4276 - accuracy: 0.0000e+00\n",
      "Epoch 77/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 744.7663 - accuracy: 0.0000e+00\n",
      "Epoch 78/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 730.0749 - accuracy: 0.0000e+00\n",
      "Epoch 79/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 715.3594 - accuracy: 0.0000e+00\n",
      "Epoch 80/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 700.6261 - accuracy: 0.0000e+00\n",
      "Epoch 81/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 685.8815 - accuracy: 0.0000e+00\n",
      "Epoch 82/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 671.1331 - accuracy: 0.0000e+00\n",
      "Epoch 83/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 656.3871 - accuracy: 0.0000e+00\n",
      "Epoch 84/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 641.6508 - accuracy: 0.0000e+00\n",
      "Epoch 85/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 626.9316 - accuracy: 0.0000e+00\n",
      "Epoch 86/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 612.2372 - accuracy: 0.0000e+00\n",
      "Epoch 87/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 597.5755 - accuracy: 0.0000e+00\n",
      "Epoch 88/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 582.9543 - accuracy: 0.0000e+00\n",
      "Epoch 89/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 568.3820 - accuracy: 0.0000e+00\n",
      "Epoch 90/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 553.8670 - accuracy: 0.0000e+00\n",
      "Epoch 91/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 539.4182 - accuracy: 0.0000e+00\n",
      "Epoch 92/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 525.0438 - accuracy: 0.0000e+00\n",
      "Epoch 93/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 510.7526 - accuracy: 0.0000e+00\n",
      "Epoch 94/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 496.5542 - accuracy: 0.0000e+00\n",
      "Epoch 95/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 482.4574 - accuracy: 0.0000e+00\n",
      "Epoch 96/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 468.4710 - accuracy: 0.0000e+00\n",
      "Epoch 97/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 454.6043 - accuracy: 0.0000e+00\n",
      "Epoch 98/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 440.8669 - accuracy: 0.0000e+00\n",
      "Epoch 99/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 427.2677 - accuracy: 0.0000e+00\n",
      "Epoch 100/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 413.8159 - accuracy: 0.0000e+00\n",
      "Epoch 101/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 400.5206 - accuracy: 0.0000e+00\n",
      "Epoch 102/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 387.3910 - accuracy: 0.0000e+00\n",
      "Epoch 103/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 374.4363 - accuracy: 0.0000e+00\n",
      "Epoch 104/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 361.6656 - accuracy: 0.0000e+00\n",
      "Epoch 105/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 349.0874 - accuracy: 0.0000e+00\n",
      "Epoch 106/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 336.7109 - accuracy: 0.0000e+00\n",
      "Epoch 107/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 324.5445 - accuracy: 0.0000e+00\n",
      "Epoch 108/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 312.5966 - accuracy: 0.0000e+00\n",
      "Epoch 109/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 300.8755 - accuracy: 0.0000e+00\n",
      "Epoch 110/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 289.3889 - accuracy: 0.0000e+00\n",
      "Epoch 111/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 278.1444 - accuracy: 0.0000e+00\n",
      "Epoch 112/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 267.1493 - accuracy: 0.0000e+00\n",
      "Epoch 113/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 256.4103 - accuracy: 0.0000e+00\n",
      "Epoch 114/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 245.9341 - accuracy: 0.0000e+00\n",
      "Epoch 115/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 235.7271 - accuracy: 0.0000e+00\n",
      "Epoch 116/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 225.7948 - accuracy: 0.0000e+00\n",
      "Epoch 117/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 216.1421 - accuracy: 0.0000e+00\n",
      "Epoch 118/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 206.7737 - accuracy: 0.0000e+00\n",
      "Epoch 119/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 197.6935 - accuracy: 0.0000e+00\n",
      "Epoch 120/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 188.9053 - accuracy: 0.0000e+00\n",
      "Epoch 121/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 180.4120 - accuracy: 0.0000e+00\n",
      "Epoch 122/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 172.2161 - accuracy: 0.0000e+00\n",
      "Epoch 123/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 164.3193 - accuracy: 0.0000e+00\n",
      "Epoch 124/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 156.7228 - accuracy: 0.0000e+00\n",
      "Epoch 125/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 149.4273 - accuracy: 0.0000e+00\n",
      "Epoch 126/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 142.4326 - accuracy: 0.0000e+00\n",
      "Epoch 127/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 135.7380 - accuracy: 0.0000e+00\n",
      "Epoch 128/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 129.3422 - accuracy: 0.0000e+00\n",
      "Epoch 129/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 123.2429 - accuracy: 0.0000e+00\n",
      "Epoch 130/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 117.4376 - accuracy: 0.0000e+00\n",
      "Epoch 131/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 111.9230 - accuracy: 0.0000e+00\n",
      "Epoch 132/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 106.6949 - accuracy: 0.0000e+00\n",
      "Epoch 133/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 101.7489 - accuracy: 0.0000e+00\n",
      "Epoch 134/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 97.0797 - accuracy: 0.0000e+00\n",
      "Epoch 135/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 92.6815 - accuracy: 0.0000e+00\n",
      "Epoch 136/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 88.5481 - accuracy: 0.0000e+00\n",
      "Epoch 137/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 84.6723 - accuracy: 0.0000e+00\n",
      "Epoch 138/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 81.0470 - accuracy: 0.0000e+00\n",
      "Epoch 139/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 77.6644 - accuracy: 0.0000e+00\n",
      "Epoch 140/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 74.5163 - accuracy: 0.0000e+00\n",
      "Epoch 141/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 71.5943 - accuracy: 0.0000e+00\n",
      "Epoch 142/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 68.8895 - accuracy: 0.0000e+00\n",
      "Epoch 143/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 66.3927 - accuracy: 0.0000e+00\n",
      "Epoch 144/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 64.0946 - accuracy: 0.0000e+00\n",
      "Epoch 145/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 61.9856 - accuracy: 0.0000e+00\n",
      "Epoch 146/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 60.0562 - accuracy: 0.0000e+00\n",
      "Epoch 147/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 58.2966 - accuracy: 0.0000e+00\n",
      "Epoch 148/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 56.6971 - accuracy: 0.0000e+00\n",
      "Epoch 149/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 55.2480 - accuracy: 0.0000e+00\n",
      "Epoch 150/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 53.9398 - accuracy: 0.0000e+00\n",
      "Epoch 151/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 52.7629 - accuracy: 0.0000e+00\n",
      "Epoch 152/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 51.7081 - accuracy: 0.0000e+00\n",
      "Epoch 153/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 50.7662 - accuracy: 0.0000e+00\n",
      "Epoch 154/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 49.9286 - accuracy: 0.0000e+00\n",
      "Epoch 155/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 49.1865 - accuracy: 0.0000e+00\n",
      "Epoch 156/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 48.5318 - accuracy: 0.0000e+00\n",
      "Epoch 157/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 47.9567 - accuracy: 0.0000e+00\n",
      "Epoch 158/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 47.4537 - accuracy: 0.0000e+00\n",
      "Epoch 159/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 47.0155 - accuracy: 0.0000e+00\n",
      "Epoch 160/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 46.6356 - accuracy: 0.0000e+00\n",
      "Epoch 161/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 46.3076 - accuracy: 0.0000e+00\n",
      "Epoch 162/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 46.0255 - accuracy: 0.0000e+00\n",
      "Epoch 163/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 45.7841 - accuracy: 0.0000e+00\n",
      "Epoch 164/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 45.5781 - accuracy: 0.0000e+00\n",
      "Epoch 165/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 45.4030 - accuracy: 0.0000e+00\n",
      "Epoch 166/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 45.2545 - accuracy: 0.0000e+00\n",
      "Epoch 167/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 45.1288 - accuracy: 0.0000e+00\n",
      "Epoch 168/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 45.0224 - accuracy: 0.0000e+00\n",
      "Epoch 169/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 44.9321 - accuracy: 0.0000e+00\n",
      "Epoch 170/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 44.8552 - accuracy: 0.0000e+00\n",
      "Epoch 171/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 44.7892 - accuracy: 0.0000e+00\n",
      "Epoch 172/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 44.7320 - accuracy: 0.0000e+00\n",
      "Epoch 173/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 44.6817 - accuracy: 0.0000e+00\n",
      "Epoch 174/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 44.6367 - accuracy: 0.0000e+00\n",
      "Epoch 175/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 44.5956 - accuracy: 0.0000e+00\n",
      "Epoch 176/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 44.5572 - accuracy: 0.0000e+00\n",
      "Epoch 177/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 44.5206 - accuracy: 0.0000e+00\n",
      "Epoch 178/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 44.4849 - accuracy: 0.0000e+00\n",
      "Epoch 179/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 44.4495 - accuracy: 0.0000e+00\n",
      "Epoch 180/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 44.4140 - accuracy: 0.0000e+00\n",
      "Epoch 181/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 44.3779 - accuracy: 0.0000e+00\n",
      "Epoch 182/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 44.3410 - accuracy: 0.0000e+00\n",
      "Epoch 183/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 44.3029 - accuracy: 0.0000e+00\n",
      "Epoch 184/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 44.2637 - accuracy: 0.0000e+00\n",
      "Epoch 185/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 44.2233 - accuracy: 0.0000e+00\n",
      "Epoch 186/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 44.1817 - accuracy: 0.0000e+00\n",
      "Epoch 187/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 44.1387 - accuracy: 0.0000e+00\n",
      "Epoch 188/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 44.0947 - accuracy: 0.0000e+00\n",
      "Epoch 189/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 44.0495 - accuracy: 0.0000e+00\n",
      "Epoch 190/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 44.0034 - accuracy: 0.0000e+00\n",
      "Epoch 191/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 43.9563 - accuracy: 0.0000e+00\n",
      "Epoch 192/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 43.9085 - accuracy: 0.0000e+00\n",
      "Epoch 193/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 43.8600 - accuracy: 0.0000e+00\n",
      "Epoch 194/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 43.8110 - accuracy: 0.0000e+00\n",
      "Epoch 195/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 43.7615 - accuracy: 0.0000e+00\n",
      "Epoch 196/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 43.7117 - accuracy: 0.0000e+00\n",
      "Epoch 197/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 43.6617 - accuracy: 0.0000e+00\n",
      "Epoch 198/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 43.6115 - accuracy: 0.0000e+00\n",
      "Epoch 199/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 43.5612 - accuracy: 0.0000e+00\n",
      "Epoch 200/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 43.5109 - accuracy: 0.0000e+00\n",
      "Epoch 201/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 43.4607 - accuracy: 0.0000e+00\n",
      "Epoch 202/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 43.4105 - accuracy: 0.0000e+00\n",
      "Epoch 203/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 43.3605 - accuracy: 0.0000e+00\n",
      "Epoch 204/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 43.3107 - accuracy: 0.0000e+00\n",
      "Epoch 205/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 43.2610 - accuracy: 0.0000e+00\n",
      "Epoch 206/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 43.2115 - accuracy: 0.0000e+00\n",
      "Epoch 207/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 43.1623 - accuracy: 0.0000e+00\n",
      "Epoch 208/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 43.1133 - accuracy: 0.0000e+00\n",
      "Epoch 209/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 43.0645 - accuracy: 0.0000e+00\n",
      "Epoch 210/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 43.0160 - accuracy: 0.0000e+00\n",
      "Epoch 211/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 42.9676 - accuracy: 0.0000e+00\n",
      "Epoch 212/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 42.9195 - accuracy: 0.0000e+00\n",
      "Epoch 213/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 42.8716 - accuracy: 0.0000e+00\n",
      "Epoch 214/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 42.8238 - accuracy: 0.0000e+00\n",
      "Epoch 215/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 42.7762 - accuracy: 0.0000e+00\n",
      "Epoch 216/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 42.7288 - accuracy: 0.0000e+00\n",
      "Epoch 217/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 42.6815 - accuracy: 0.0000e+00\n",
      "Epoch 218/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 42.6344 - accuracy: 0.0000e+00\n",
      "Epoch 219/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 42.5873 - accuracy: 0.0000e+00\n",
      "Epoch 220/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 42.5404 - accuracy: 0.0000e+00\n",
      "Epoch 221/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 42.4935 - accuracy: 0.0000e+00\n",
      "Epoch 222/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 42.4467 - accuracy: 0.0000e+00\n",
      "Epoch 223/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 42.3999 - accuracy: 0.0000e+00\n",
      "Epoch 224/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 42.3532 - accuracy: 0.0000e+00\n",
      "Epoch 225/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 42.3066 - accuracy: 0.0000e+00\n",
      "Epoch 226/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 42.2599 - accuracy: 0.0000e+00\n",
      "Epoch 227/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 42.2133 - accuracy: 0.0000e+00\n",
      "Epoch 228/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 42.1667 - accuracy: 0.0000e+00\n",
      "Epoch 229/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 42.1201 - accuracy: 0.0000e+00\n",
      "Epoch 230/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 42.0735 - accuracy: 0.0000e+00\n",
      "Epoch 231/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 42.0269 - accuracy: 0.0000e+00\n",
      "Epoch 232/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 41.9803 - accuracy: 0.0000e+00\n",
      "Epoch 233/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 41.9337 - accuracy: 0.0000e+00\n",
      "Epoch 234/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 41.8870 - accuracy: 0.0000e+00\n",
      "Epoch 235/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 41.8403 - accuracy: 0.0000e+00\n",
      "Epoch 236/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 41.7937 - accuracy: 0.0000e+00\n",
      "Epoch 237/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 41.7470 - accuracy: 0.0000e+00\n",
      "Epoch 238/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 41.7003 - accuracy: 0.0000e+00\n",
      "Epoch 239/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 41.6536 - accuracy: 0.0000e+00\n",
      "Epoch 240/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 41.6068 - accuracy: 0.0000e+00\n",
      "Epoch 241/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 41.5600 - accuracy: 0.0000e+00\n",
      "Epoch 242/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 41.5133 - accuracy: 0.0000e+00\n",
      "Epoch 243/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 41.4665 - accuracy: 0.0000e+00\n",
      "Epoch 244/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 41.4196 - accuracy: 0.0000e+00\n",
      "Epoch 245/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 41.3728 - accuracy: 0.0000e+00\n",
      "Epoch 246/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 41.3260 - accuracy: 0.0000e+00\n",
      "Epoch 247/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 41.2791 - accuracy: 0.0000e+00\n",
      "Epoch 248/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 41.2323 - accuracy: 0.0000e+00\n",
      "Epoch 249/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 41.1854 - accuracy: 0.0000e+00\n",
      "Epoch 250/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 41.1385 - accuracy: 0.0000e+00\n",
      "Epoch 251/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 41.0916 - accuracy: 0.0000e+00\n",
      "Epoch 252/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 41.0447 - accuracy: 0.0000e+00\n",
      "Epoch 253/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 40.9978 - accuracy: 0.0000e+00\n",
      "Epoch 254/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 40.9508 - accuracy: 0.0000e+00\n",
      "Epoch 255/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 40.9039 - accuracy: 0.0000e+00\n",
      "Epoch 256/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 40.8570 - accuracy: 0.0000e+00\n",
      "Epoch 257/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 40.8100 - accuracy: 0.0000e+00\n",
      "Epoch 258/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 40.7631 - accuracy: 0.0000e+00\n",
      "Epoch 259/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 40.7161 - accuracy: 0.0000e+00\n",
      "Epoch 260/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 40.6692 - accuracy: 0.0000e+00\n",
      "Epoch 261/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 40.6222 - accuracy: 0.0000e+00\n",
      "Epoch 262/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 40.5752 - accuracy: 0.0000e+00\n",
      "Epoch 263/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 40.5283 - accuracy: 0.0000e+00\n",
      "Epoch 264/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 40.4813 - accuracy: 0.0000e+00\n",
      "Epoch 265/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 40.4343 - accuracy: 0.0000e+00\n",
      "Epoch 266/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 40.3874 - accuracy: 0.0000e+00\n",
      "Epoch 267/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 40.3404 - accuracy: 0.0000e+00\n",
      "Epoch 268/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 40.2934 - accuracy: 0.0000e+00\n",
      "Epoch 269/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 40.2465 - accuracy: 0.0000e+00\n",
      "Epoch 270/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 40.1995 - accuracy: 0.0000e+00\n",
      "Epoch 271/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 40.1525 - accuracy: 0.0000e+00\n",
      "Epoch 272/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 40.1055 - accuracy: 0.0000e+00\n",
      "Epoch 273/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 40.0586 - accuracy: 0.0000e+00\n",
      "Epoch 274/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 40.0116 - accuracy: 0.0000e+00\n",
      "Epoch 275/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 39.9646 - accuracy: 0.0000e+00\n",
      "Epoch 276/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 39.9177 - accuracy: 0.0000e+00\n",
      "Epoch 277/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 39.8707 - accuracy: 0.0000e+00\n",
      "Epoch 278/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 39.8237 - accuracy: 0.0000e+00\n",
      "Epoch 279/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 39.7768 - accuracy: 0.0000e+00\n",
      "Epoch 280/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 39.7298 - accuracy: 0.0000e+00\n",
      "Epoch 281/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 39.6829 - accuracy: 0.0000e+00\n",
      "Epoch 282/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 39.6359 - accuracy: 0.0000e+00\n",
      "Epoch 283/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 39.5890 - accuracy: 0.0000e+00\n",
      "Epoch 284/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 39.5420 - accuracy: 0.0000e+00\n",
      "Epoch 285/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 39.4951 - accuracy: 0.0000e+00\n",
      "Epoch 286/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 39.4481 - accuracy: 0.0000e+00\n",
      "Epoch 287/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 39.4012 - accuracy: 0.0000e+00\n",
      "Epoch 288/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 39.3543 - accuracy: 0.0000e+00\n",
      "Epoch 289/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 39.3074 - accuracy: 0.0000e+00\n",
      "Epoch 290/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 39.2604 - accuracy: 0.0000e+00\n",
      "Epoch 291/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 39.2135 - accuracy: 0.0000e+00\n",
      "Epoch 292/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 39.1666 - accuracy: 0.0000e+00\n",
      "Epoch 293/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 39.1197 - accuracy: 0.0000e+00\n",
      "Epoch 294/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 39.0729 - accuracy: 0.0000e+00\n",
      "Epoch 295/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 39.0260 - accuracy: 0.0000e+00\n",
      "Epoch 296/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 38.9791 - accuracy: 0.0000e+00\n",
      "Epoch 297/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 38.9322 - accuracy: 0.0000e+00\n",
      "Epoch 298/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 38.8854 - accuracy: 0.0000e+00\n",
      "Epoch 299/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 38.8386 - accuracy: 0.0000e+00\n",
      "Epoch 300/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 38.7917 - accuracy: 0.0000e+00\n",
      "Epoch 301/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 38.7449 - accuracy: 0.0000e+00\n",
      "Epoch 302/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 38.6981 - accuracy: 0.0000e+00\n",
      "Epoch 303/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 38.6513 - accuracy: 0.0000e+00\n",
      "Epoch 304/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 38.6045 - accuracy: 0.0000e+00\n",
      "Epoch 305/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 38.5577 - accuracy: 0.0000e+00\n",
      "Epoch 306/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 38.5109 - accuracy: 0.0000e+00\n",
      "Epoch 307/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 38.4642 - accuracy: 0.0000e+00\n",
      "Epoch 308/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 38.4174 - accuracy: 0.0000e+00\n",
      "Epoch 309/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 38.3707 - accuracy: 0.0000e+00\n",
      "Epoch 310/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 38.3240 - accuracy: 0.0000e+00\n",
      "Epoch 311/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 38.2773 - accuracy: 0.0000e+00\n",
      "Epoch 312/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 38.2306 - accuracy: 0.0000e+00\n",
      "Epoch 313/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 38.1839 - accuracy: 0.0000e+00\n",
      "Epoch 314/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 38.1372 - accuracy: 0.0000e+00\n",
      "Epoch 315/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 38.0906 - accuracy: 0.0000e+00\n",
      "Epoch 316/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 38.0439 - accuracy: 0.0000e+00\n",
      "Epoch 317/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 37.9973 - accuracy: 0.0000e+00\n",
      "Epoch 318/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 37.9507 - accuracy: 0.0000e+00\n",
      "Epoch 319/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 37.9041 - accuracy: 0.0000e+00\n",
      "Epoch 320/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 37.8575 - accuracy: 0.0000e+00\n",
      "Epoch 321/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 37.8110 - accuracy: 0.0000e+00\n",
      "Epoch 322/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 37.7644 - accuracy: 0.0000e+00\n",
      "Epoch 323/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 37.7179 - accuracy: 0.0000e+00\n",
      "Epoch 324/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 37.6714 - accuracy: 0.0000e+00\n",
      "Epoch 325/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 37.6249 - accuracy: 0.0000e+00\n",
      "Epoch 326/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 37.5784 - accuracy: 0.0000e+00\n",
      "Epoch 327/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 37.5319 - accuracy: 0.0000e+00\n",
      "Epoch 328/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 37.4855 - accuracy: 0.0000e+00\n",
      "Epoch 329/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 37.4390 - accuracy: 0.0000e+00\n",
      "Epoch 330/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 37.3926 - accuracy: 0.0000e+00\n",
      "Epoch 331/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 37.3462 - accuracy: 0.0000e+00\n",
      "Epoch 332/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 37.2998 - accuracy: 0.0000e+00\n",
      "Epoch 333/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 37.2535 - accuracy: 0.0000e+00\n",
      "Epoch 334/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 37.2071 - accuracy: 0.0000e+00\n",
      "Epoch 335/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 37.1608 - accuracy: 0.0000e+00\n",
      "Epoch 336/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 37.1145 - accuracy: 0.0000e+00\n",
      "Epoch 337/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 37.0683 - accuracy: 0.0000e+00\n",
      "Epoch 338/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 37.0220 - accuracy: 0.0000e+00\n",
      "Epoch 339/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 36.9758 - accuracy: 0.0000e+00\n",
      "Epoch 340/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 36.9296 - accuracy: 0.0000e+00\n",
      "Epoch 341/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 36.8833 - accuracy: 0.0000e+00\n",
      "Epoch 342/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 36.8372 - accuracy: 0.0000e+00\n",
      "Epoch 343/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 36.7910 - accuracy: 0.0000e+00\n",
      "Epoch 344/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 36.7449 - accuracy: 0.0000e+00\n",
      "Epoch 345/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 36.6988 - accuracy: 0.0000e+00\n",
      "Epoch 346/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 36.6527 - accuracy: 0.0000e+00\n",
      "Epoch 347/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 36.6066 - accuracy: 0.0000e+00\n",
      "Epoch 348/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 36.5606 - accuracy: 0.0000e+00\n",
      "Epoch 349/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 36.5145 - accuracy: 0.0000e+00\n",
      "Epoch 350/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 36.4685 - accuracy: 0.0000e+00\n",
      "Epoch 351/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 36.4225 - accuracy: 0.0000e+00\n",
      "Epoch 352/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 36.3766 - accuracy: 0.0000e+00\n",
      "Epoch 353/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 36.3307 - accuracy: 0.0000e+00\n",
      "Epoch 354/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 36.2847 - accuracy: 0.0000e+00\n",
      "Epoch 355/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 36.2389 - accuracy: 0.0000e+00\n",
      "Epoch 356/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 36.1930 - accuracy: 0.0000e+00\n",
      "Epoch 357/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 36.1472 - accuracy: 0.0000e+00\n",
      "Epoch 358/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 36.1013 - accuracy: 0.0000e+00\n",
      "Epoch 359/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 36.0556 - accuracy: 0.0000e+00\n",
      "Epoch 360/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 36.0098 - accuracy: 0.0000e+00\n",
      "Epoch 361/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 35.9641 - accuracy: 0.0000e+00\n",
      "Epoch 362/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 35.9184 - accuracy: 0.0000e+00\n",
      "Epoch 363/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 35.8727 - accuracy: 0.0000e+00\n",
      "Epoch 364/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 35.8270 - accuracy: 0.0000e+00\n",
      "Epoch 365/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 35.7814 - accuracy: 0.0000e+00\n",
      "Epoch 366/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 35.7358 - accuracy: 0.0000e+00\n",
      "Epoch 367/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 35.6902 - accuracy: 0.0000e+00\n",
      "Epoch 368/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 35.6446 - accuracy: 0.0000e+00\n",
      "Epoch 369/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 35.5991 - accuracy: 0.0000e+00\n",
      "Epoch 370/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 35.5536 - accuracy: 0.0000e+00\n",
      "Epoch 371/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 35.5081 - accuracy: 0.0000e+00\n",
      "Epoch 372/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 35.4627 - accuracy: 0.0000e+00\n",
      "Epoch 373/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 35.4172 - accuracy: 0.0000e+00\n",
      "Epoch 374/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 35.3718 - accuracy: 0.0000e+00\n",
      "Epoch 375/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 35.3265 - accuracy: 0.0000e+00\n",
      "Epoch 376/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 35.2812 - accuracy: 0.0000e+00\n",
      "Epoch 377/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 35.2358 - accuracy: 0.0000e+00\n",
      "Epoch 378/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 35.1905 - accuracy: 0.0000e+00\n",
      "Epoch 379/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 35.1453 - accuracy: 0.0000e+00\n",
      "Epoch 380/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 35.1001 - accuracy: 0.0000e+00\n",
      "Epoch 381/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 35.0549 - accuracy: 0.0000e+00\n",
      "Epoch 382/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 35.0097 - accuracy: 0.0000e+00\n",
      "Epoch 383/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 34.9646 - accuracy: 0.0000e+00\n",
      "Epoch 384/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 34.9195 - accuracy: 0.0000e+00\n",
      "Epoch 385/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 34.8744 - accuracy: 0.0000e+00\n",
      "Epoch 386/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 34.8294 - accuracy: 0.0000e+00\n",
      "Epoch 387/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 34.7843 - accuracy: 0.0000e+00\n",
      "Epoch 388/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 34.7393 - accuracy: 0.0000e+00\n",
      "Epoch 389/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 34.6944 - accuracy: 0.0000e+00\n",
      "Epoch 390/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 34.6494 - accuracy: 0.0000e+00\n",
      "Epoch 391/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 34.6045 - accuracy: 0.0000e+00\n",
      "Epoch 392/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 34.5597 - accuracy: 0.0000e+00\n",
      "Epoch 393/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 34.5148 - accuracy: 0.0000e+00\n",
      "Epoch 394/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 34.4701 - accuracy: 0.0000e+00\n",
      "Epoch 395/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 34.4253 - accuracy: 0.0000e+00\n",
      "Epoch 396/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 34.3806 - accuracy: 0.0000e+00\n",
      "Epoch 397/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 34.3358 - accuracy: 0.0000e+00\n",
      "Epoch 398/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 34.2912 - accuracy: 0.0000e+00\n",
      "Epoch 399/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 34.2465 - accuracy: 0.0000e+00\n",
      "Epoch 400/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 34.2019 - accuracy: 0.0000e+00\n",
      "Epoch 401/1000\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 34.1574 - accuracy: 0.0000e+00\n",
      "Epoch 402/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 34.1128 - accuracy: 0.0000e+00\n",
      "Epoch 403/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 34.0683 - accuracy: 0.0000e+00\n",
      "Epoch 404/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 34.0238 - accuracy: 0.0000e+00\n",
      "Epoch 405/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 33.9794 - accuracy: 0.0000e+00\n",
      "Epoch 406/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 33.9350 - accuracy: 0.0000e+00\n",
      "Epoch 407/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 33.8906 - accuracy: 0.0000e+00\n",
      "Epoch 408/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 33.8463 - accuracy: 0.0000e+00\n",
      "Epoch 409/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 33.8020 - accuracy: 0.0000e+00\n",
      "Epoch 410/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 33.7577 - accuracy: 0.0000e+00\n",
      "Epoch 411/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 33.7135 - accuracy: 0.0000e+00\n",
      "Epoch 412/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 33.6693 - accuracy: 0.0000e+00\n",
      "Epoch 413/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 33.6251 - accuracy: 0.0000e+00\n",
      "Epoch 414/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 33.5810 - accuracy: 0.0000e+00\n",
      "Epoch 415/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 33.5369 - accuracy: 0.0000e+00\n",
      "Epoch 416/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 33.4928 - accuracy: 0.0000e+00\n",
      "Epoch 417/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 33.4488 - accuracy: 0.0000e+00\n",
      "Epoch 418/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 33.4048 - accuracy: 0.0000e+00\n",
      "Epoch 419/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 33.3608 - accuracy: 0.0000e+00\n",
      "Epoch 420/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 33.3139 - accuracy: 0.0000e+00\n",
      "Epoch 421/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 33.2659 - accuracy: 0.0000e+00\n",
      "Epoch 422/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 33.2061 - accuracy: 0.0000e+00\n",
      "Epoch 423/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 33.1040 - accuracy: 0.0000e+00\n",
      "Epoch 424/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 32.9975 - accuracy: 0.0000e+00\n",
      "Epoch 425/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 32.9515 - accuracy: 0.0000e+00\n",
      "Epoch 426/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 32.9056 - accuracy: 0.0000e+00\n",
      "Epoch 427/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 32.8589 - accuracy: 0.0000e+00\n",
      "Epoch 428/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 32.8109 - accuracy: 0.0000e+00\n",
      "Epoch 429/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 32.7613 - accuracy: 0.0000e+00\n",
      "Epoch 430/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 32.7098 - accuracy: 0.0000e+00\n",
      "Epoch 431/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 32.6563 - accuracy: 0.0000e+00\n",
      "Epoch 432/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 32.6010 - accuracy: 0.0000e+00\n",
      "Epoch 433/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 32.5441 - accuracy: 0.0000e+00\n",
      "Epoch 434/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 32.4859 - accuracy: 0.0000e+00\n",
      "Epoch 435/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 32.4269 - accuracy: 0.0000e+00\n",
      "Epoch 436/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 32.3675 - accuracy: 0.0000e+00\n",
      "Epoch 437/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 32.3079 - accuracy: 0.0000e+00\n",
      "Epoch 438/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 32.2486 - accuracy: 0.0000e+00\n",
      "Epoch 439/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 32.1897 - accuracy: 0.0000e+00\n",
      "Epoch 440/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 32.1312 - accuracy: 0.0000e+00\n",
      "Epoch 441/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 32.0731 - accuracy: 0.0000e+00\n",
      "Epoch 442/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 32.0154 - accuracy: 0.0000e+00\n",
      "Epoch 443/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 31.9580 - accuracy: 0.0000e+00\n",
      "Epoch 444/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 31.9005 - accuracy: 0.0000e+00\n",
      "Epoch 445/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 31.8430 - accuracy: 0.0000e+00\n",
      "Epoch 446/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 31.7851 - accuracy: 0.0000e+00\n",
      "Epoch 447/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 31.7267 - accuracy: 0.0000e+00\n",
      "Epoch 448/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 31.6740 - accuracy: 0.0000e+00\n",
      "Epoch 449/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 31.6081 - accuracy: 0.0000e+00\n",
      "Epoch 450/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 31.5499 - accuracy: 0.0000e+00\n",
      "Epoch 451/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 31.4932 - accuracy: 0.0000e+00\n",
      "Epoch 452/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 31.4374 - accuracy: 0.0000e+00\n",
      "Epoch 453/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 31.3822 - accuracy: 0.0000e+00\n",
      "Epoch 454/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 31.3271 - accuracy: 0.0000e+00\n",
      "Epoch 455/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 31.2715 - accuracy: 0.0000e+00\n",
      "Epoch 456/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 31.2153 - accuracy: 0.0000e+00\n",
      "Epoch 457/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 31.1582 - accuracy: 0.0000e+00\n",
      "Epoch 458/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 31.1001 - accuracy: 0.0000e+00\n",
      "Epoch 459/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 31.0411 - accuracy: 0.0000e+00\n",
      "Epoch 460/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 30.9813 - accuracy: 0.0000e+00\n",
      "Epoch 461/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 30.9208 - accuracy: 0.0000e+00\n",
      "Epoch 462/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 30.8600 - accuracy: 0.0000e+00\n",
      "Epoch 463/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 30.7991 - accuracy: 0.0000e+00\n",
      "Epoch 464/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 30.7383 - accuracy: 0.0000e+00\n",
      "Epoch 465/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 30.6777 - accuracy: 0.0000e+00\n",
      "Epoch 466/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 30.6174 - accuracy: 0.0000e+00\n",
      "Epoch 467/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 30.5573 - accuracy: 0.0000e+00\n",
      "Epoch 468/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 30.4976 - accuracy: 0.0000e+00\n",
      "Epoch 469/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 30.4382 - accuracy: 0.0000e+00\n",
      "Epoch 470/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 30.3788 - accuracy: 0.0000e+00\n",
      "Epoch 471/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 30.3195 - accuracy: 0.0000e+00\n",
      "Epoch 472/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 30.2601 - accuracy: 0.0000e+00\n",
      "Epoch 473/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 30.2005 - accuracy: 0.0000e+00\n",
      "Epoch 474/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 30.1407 - accuracy: 0.0000e+00\n",
      "Epoch 475/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 30.0806 - accuracy: 0.0000e+00\n",
      "Epoch 476/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 30.0203 - accuracy: 0.0000e+00\n",
      "Epoch 477/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 29.9597 - accuracy: 0.0000e+00\n",
      "Epoch 478/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 29.8990 - accuracy: 0.0000e+00\n",
      "Epoch 479/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 29.8383 - accuracy: 0.0000e+00\n",
      "Epoch 480/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 29.7774 - accuracy: 0.0000e+00\n",
      "Epoch 481/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 29.7166 - accuracy: 0.0000e+00\n",
      "Epoch 482/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 29.6557 - accuracy: 0.0000e+00\n",
      "Epoch 483/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 29.5950 - accuracy: 0.0000e+00\n",
      "Epoch 484/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 29.5342 - accuracy: 0.0000e+00\n",
      "Epoch 485/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 29.4734 - accuracy: 0.0000e+00\n",
      "Epoch 486/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 29.4126 - accuracy: 0.0000e+00\n",
      "Epoch 487/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 29.3518 - accuracy: 0.0000e+00\n",
      "Epoch 488/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 29.2908 - accuracy: 0.0000e+00\n",
      "Epoch 489/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 29.2298 - accuracy: 0.0000e+00\n",
      "Epoch 490/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 29.1686 - accuracy: 0.0000e+00\n",
      "Epoch 491/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 29.1074 - accuracy: 0.0000e+00\n",
      "Epoch 492/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 29.0460 - accuracy: 0.0000e+00\n",
      "Epoch 493/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 28.9846 - accuracy: 0.0000e+00\n",
      "Epoch 494/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 28.9231 - accuracy: 0.0000e+00\n",
      "Epoch 495/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 28.8615 - accuracy: 0.0000e+00\n",
      "Epoch 496/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 28.7999 - accuracy: 0.0000e+00\n",
      "Epoch 497/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 28.7383 - accuracy: 0.0000e+00\n",
      "Epoch 498/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 28.6767 - accuracy: 0.0000e+00\n",
      "Epoch 499/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 28.6150 - accuracy: 0.0000e+00\n",
      "Epoch 500/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 28.5533 - accuracy: 0.0000e+00\n",
      "Epoch 501/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 28.4915 - accuracy: 0.0000e+00\n",
      "Epoch 502/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 28.4297 - accuracy: 0.0000e+00\n",
      "Epoch 503/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 28.3678 - accuracy: 0.0000e+00\n",
      "Epoch 504/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 28.3059 - accuracy: 0.0000e+00\n",
      "Epoch 505/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 28.2439 - accuracy: 0.0000e+00\n",
      "Epoch 506/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 28.1818 - accuracy: 0.0000e+00\n",
      "Epoch 507/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 28.1197 - accuracy: 0.0000e+00\n",
      "Epoch 508/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 28.0576 - accuracy: 0.0000e+00\n",
      "Epoch 509/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 27.9954 - accuracy: 0.0000e+00\n",
      "Epoch 510/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 27.9332 - accuracy: 0.0000e+00\n",
      "Epoch 511/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 27.8709 - accuracy: 0.0000e+00\n",
      "Epoch 512/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 27.8086 - accuracy: 0.0000e+00\n",
      "Epoch 513/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 27.7463 - accuracy: 0.0000e+00\n",
      "Epoch 514/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 27.6840 - accuracy: 0.0000e+00\n",
      "Epoch 515/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 27.6216 - accuracy: 0.0000e+00\n",
      "Epoch 516/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 27.5591 - accuracy: 0.0000e+00\n",
      "Epoch 517/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 27.5032 - accuracy: 0.0000e+00\n",
      "Epoch 518/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 27.4395 - accuracy: 0.0000e+00\n",
      "Epoch 519/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 27.3861 - accuracy: 0.0000e+00\n",
      "Epoch 520/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 27.3341 - accuracy: 0.0000e+00\n",
      "Epoch 521/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 27.2814 - accuracy: 0.0000e+00\n",
      "Epoch 522/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 27.2267 - accuracy: 0.0000e+00\n",
      "Epoch 523/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 27.1689 - accuracy: 0.0000e+00\n",
      "Epoch 524/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 27.1079 - accuracy: 0.0000e+00\n",
      "Epoch 525/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 27.0440 - accuracy: 0.0000e+00\n",
      "Epoch 526/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 26.9783 - accuracy: 0.0000e+00\n",
      "Epoch 527/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 26.9116 - accuracy: 0.0000e+00\n",
      "Epoch 528/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 26.8451 - accuracy: 0.0000e+00\n",
      "Epoch 529/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 26.7795 - accuracy: 0.0000e+00\n",
      "Epoch 530/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 26.7154 - accuracy: 0.0000e+00\n",
      "Epoch 531/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 26.6529 - accuracy: 0.0000e+00\n",
      "Epoch 532/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 26.5921 - accuracy: 0.0000e+00\n",
      "Epoch 533/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 26.5324 - accuracy: 0.0000e+00\n",
      "Epoch 534/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 26.4734 - accuracy: 0.0000e+00\n",
      "Epoch 535/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 26.4370 - accuracy: 0.0000e+00\n",
      "Epoch 536/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 26.3521 - accuracy: 0.0000e+00\n",
      "Epoch 537/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 26.2918 - accuracy: 0.0000e+00\n",
      "Epoch 538/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 26.2335 - accuracy: 0.0000e+00\n",
      "Epoch 539/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 26.1765 - accuracy: 0.0000e+00\n",
      "Epoch 540/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 26.1200 - accuracy: 0.0000e+00\n",
      "Epoch 541/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 26.0635 - accuracy: 0.0000e+00\n",
      "Epoch 542/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 26.0062 - accuracy: 0.0000e+00\n",
      "Epoch 543/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 25.9480 - accuracy: 0.0000e+00\n",
      "Epoch 544/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 25.8887 - accuracy: 0.0000e+00\n",
      "Epoch 545/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 25.8284 - accuracy: 0.0000e+00\n",
      "Epoch 546/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 25.7674 - accuracy: 0.0000e+00\n",
      "Epoch 547/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 25.7059 - accuracy: 0.0000e+00\n",
      "Epoch 548/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 25.6443 - accuracy: 0.0000e+00\n",
      "Epoch 549/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 25.5830 - accuracy: 0.0000e+00\n",
      "Epoch 550/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 25.5222 - accuracy: 0.0000e+00\n",
      "Epoch 551/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 25.4620 - accuracy: 0.0000e+00\n",
      "Epoch 552/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 25.4023 - accuracy: 0.0000e+00\n",
      "Epoch 553/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 25.3431 - accuracy: 0.0000e+00\n",
      "Epoch 554/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 25.2843 - accuracy: 0.0000e+00\n",
      "Epoch 555/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 25.2256 - accuracy: 0.0000e+00\n",
      "Epoch 556/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 25.1669 - accuracy: 0.0000e+00\n",
      "Epoch 557/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 25.1081 - accuracy: 0.0000e+00\n",
      "Epoch 558/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 25.0490 - accuracy: 0.0000e+00\n",
      "Epoch 559/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 24.9897 - accuracy: 0.0000e+00\n",
      "Epoch 560/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 24.9303 - accuracy: 0.0000e+00\n",
      "Epoch 561/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 24.8707 - accuracy: 0.0000e+00\n",
      "Epoch 562/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 24.8110 - accuracy: 0.0000e+00\n",
      "Epoch 563/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 24.7514 - accuracy: 0.0000e+00\n",
      "Epoch 564/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 24.6919 - accuracy: 0.0000e+00\n",
      "Epoch 565/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 24.6326 - accuracy: 0.0000e+00\n",
      "Epoch 566/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 24.5734 - accuracy: 0.0000e+00\n",
      "Epoch 567/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 24.5144 - accuracy: 0.0000e+00\n",
      "Epoch 568/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 24.4555 - accuracy: 0.0000e+00\n",
      "Epoch 569/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 24.3967 - accuracy: 0.0000e+00\n",
      "Epoch 570/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 24.3379 - accuracy: 0.0000e+00\n",
      "Epoch 571/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 24.2791 - accuracy: 0.0000e+00\n",
      "Epoch 572/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 24.2203 - accuracy: 0.0000e+00\n",
      "Epoch 573/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 24.1615 - accuracy: 0.0000e+00\n",
      "Epoch 574/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 24.1027 - accuracy: 0.0000e+00\n",
      "Epoch 575/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 24.0438 - accuracy: 0.0000e+00\n",
      "Epoch 576/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 23.9850 - accuracy: 0.0000e+00\n",
      "Epoch 577/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 23.9262 - accuracy: 0.0000e+00\n",
      "Epoch 578/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 23.8676 - accuracy: 0.0000e+00\n",
      "Epoch 579/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 23.8089 - accuracy: 0.0000e+00\n",
      "Epoch 580/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 23.7504 - accuracy: 0.0000e+00\n",
      "Epoch 581/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 23.6919 - accuracy: 0.0000e+00\n",
      "Epoch 582/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 23.6335 - accuracy: 0.0000e+00\n",
      "Epoch 583/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 23.5752 - accuracy: 0.0000e+00\n",
      "Epoch 584/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 23.5170 - accuracy: 0.0000e+00\n",
      "Epoch 585/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 23.4587 - accuracy: 0.0000e+00\n",
      "Epoch 586/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 23.4006 - accuracy: 0.0000e+00\n",
      "Epoch 587/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 23.3424 - accuracy: 0.0000e+00\n",
      "Epoch 588/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 23.2843 - accuracy: 0.0000e+00\n",
      "Epoch 589/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 23.2263 - accuracy: 0.0000e+00\n",
      "Epoch 590/1000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 23.1683 - accuracy: 0.0000e+00\n",
      "Epoch 591/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 23.1103 - accuracy: 0.0000e+00\n",
      "Epoch 592/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 23.0525 - accuracy: 0.0000e+00\n",
      "Epoch 593/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 22.9947 - accuracy: 0.0000e+00\n",
      "Epoch 594/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 22.9370 - accuracy: 0.0000e+00\n",
      "Epoch 595/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 22.8794 - accuracy: 0.0000e+00\n",
      "Epoch 596/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 22.8218 - accuracy: 0.0000e+00\n",
      "Epoch 597/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 22.7643 - accuracy: 0.0000e+00\n",
      "Epoch 598/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 22.7069 - accuracy: 0.0000e+00\n",
      "Epoch 599/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 22.6496 - accuracy: 0.0000e+00\n",
      "Epoch 600/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 22.5923 - accuracy: 0.0000e+00\n",
      "Epoch 601/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 22.5351 - accuracy: 0.0000e+00\n",
      "Epoch 602/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 22.4779 - accuracy: 0.0000e+00\n",
      "Epoch 603/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 22.4270 - accuracy: 0.0000e+00\n",
      "Epoch 604/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 22.3689 - accuracy: 0.0000e+00\n",
      "Epoch 605/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 22.3211 - accuracy: 0.0000e+00\n",
      "Epoch 606/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 22.2748 - accuracy: 0.0000e+00\n",
      "Epoch 607/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 22.2274 - accuracy: 0.0000e+00\n",
      "Epoch 608/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 22.1771 - accuracy: 0.0000e+00\n",
      "Epoch 609/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 22.1232 - accuracy: 0.0000e+00\n",
      "Epoch 610/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 22.0656 - accuracy: 0.0000e+00\n",
      "Epoch 611/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 22.0054 - accuracy: 0.0000e+00\n",
      "Epoch 612/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 21.9438 - accuracy: 0.0000e+00\n",
      "Epoch 613/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 21.8822 - accuracy: 0.0000e+00\n",
      "Epoch 614/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 21.8217 - accuracy: 0.0000e+00\n",
      "Epoch 615/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 21.7632 - accuracy: 0.0000e+00\n",
      "Epoch 616/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 21.7068 - accuracy: 0.0000e+00\n",
      "Epoch 617/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 21.6525 - accuracy: 0.0000e+00\n",
      "Epoch 618/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 21.5996 - accuracy: 0.0000e+00\n",
      "Epoch 619/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 21.5726 - accuracy: 0.0000e+00\n",
      "Epoch 620/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 21.4913 - accuracy: 0.0000e+00\n",
      "Epoch 621/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 21.4377 - accuracy: 0.0000e+00\n",
      "Epoch 622/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 21.3862 - accuracy: 0.0000e+00\n",
      "Epoch 623/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 21.3360 - accuracy: 0.0000e+00\n",
      "Epoch 624/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 21.2862 - accuracy: 0.0000e+00\n",
      "Epoch 625/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 21.2360 - accuracy: 0.0000e+00\n",
      "Epoch 626/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 21.1848 - accuracy: 0.0000e+00\n",
      "Epoch 627/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 21.1325 - accuracy: 0.0000e+00\n",
      "Epoch 628/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 21.0791 - accuracy: 0.0000e+00\n",
      "Epoch 629/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 21.0249 - accuracy: 0.0000e+00\n",
      "Epoch 630/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 20.9703 - accuracy: 0.0000e+00\n",
      "Epoch 631/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 20.9158 - accuracy: 0.0000e+00\n",
      "Epoch 632/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 20.8618 - accuracy: 0.0000e+00\n",
      "Epoch 633/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 20.8085 - accuracy: 0.0000e+00\n",
      "Epoch 634/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 20.7560 - accuracy: 0.0000e+00\n",
      "Epoch 635/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 20.7042 - accuracy: 0.0000e+00\n",
      "Epoch 636/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 20.6529 - accuracy: 0.0000e+00\n",
      "Epoch 637/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 20.6021 - accuracy: 0.0000e+00\n",
      "Epoch 638/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 20.5513 - accuracy: 0.0000e+00\n",
      "Epoch 639/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 20.5004 - accuracy: 0.0000e+00\n",
      "Epoch 640/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 20.4494 - accuracy: 0.0000e+00\n",
      "Epoch 641/1000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 20.3982 - accuracy: 0.0000e+00\n",
      "Epoch 642/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 20.3469 - accuracy: 0.0000e+00\n",
      "Epoch 643/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 20.2955 - accuracy: 0.0000e+00\n",
      "Epoch 644/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 20.2442 - accuracy: 0.0000e+00\n",
      "Epoch 645/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 20.1931 - accuracy: 0.0000e+00\n",
      "Epoch 646/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 20.1423 - accuracy: 0.0000e+00\n",
      "Epoch 647/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 20.0918 - accuracy: 0.0000e+00\n",
      "Epoch 648/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 20.0416 - accuracy: 0.0000e+00\n",
      "Epoch 649/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 19.9917 - accuracy: 0.0000e+00\n",
      "Epoch 650/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 19.9420 - accuracy: 0.0000e+00\n",
      "Epoch 651/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 19.8925 - accuracy: 0.0000e+00\n",
      "Epoch 652/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 19.8431 - accuracy: 0.0000e+00\n",
      "Epoch 653/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 19.7937 - accuracy: 0.0000e+00\n",
      "Epoch 654/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 19.7444 - accuracy: 0.0000e+00\n",
      "Epoch 655/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 19.6952 - accuracy: 0.0000e+00\n",
      "Epoch 656/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 19.6461 - accuracy: 0.0000e+00\n",
      "Epoch 657/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 19.5972 - accuracy: 0.0000e+00\n",
      "Epoch 658/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 19.5484 - accuracy: 0.0000e+00\n",
      "Epoch 659/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 19.4998 - accuracy: 0.0000e+00\n",
      "Epoch 660/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 19.4515 - accuracy: 0.0000e+00\n",
      "Epoch 661/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 19.4033 - accuracy: 0.0000e+00\n",
      "Epoch 662/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 19.3554 - accuracy: 0.0000e+00\n",
      "Epoch 663/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 19.3077 - accuracy: 0.0000e+00\n",
      "Epoch 664/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 19.2601 - accuracy: 0.0000e+00\n",
      "Epoch 665/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 19.2127 - accuracy: 0.0000e+00\n",
      "Epoch 666/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 19.1654 - accuracy: 0.0000e+00\n",
      "Epoch 667/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 19.1183 - accuracy: 0.0000e+00\n",
      "Epoch 668/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 19.0713 - accuracy: 0.0000e+00\n",
      "Epoch 669/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 19.0246 - accuracy: 0.0000e+00\n",
      "Epoch 670/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 18.9779 - accuracy: 0.0000e+00\n",
      "Epoch 671/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 18.9315 - accuracy: 0.0000e+00\n",
      "Epoch 672/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 18.8853 - accuracy: 0.0000e+00\n",
      "Epoch 673/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 18.8393 - accuracy: 0.0000e+00\n",
      "Epoch 674/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 18.7935 - accuracy: 0.0000e+00\n",
      "Epoch 675/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 18.7478 - accuracy: 0.0000e+00\n",
      "Epoch 676/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 18.7056 - accuracy: 0.0000e+00\n",
      "Epoch 677/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 18.6625 - accuracy: 0.0000e+00\n",
      "Epoch 678/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 18.6269 - accuracy: 0.0000e+00\n",
      "Epoch 679/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 18.5926 - accuracy: 0.0000e+00\n",
      "Epoch 680/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 18.5565 - accuracy: 0.0000e+00\n",
      "Epoch 681/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 18.5169 - accuracy: 0.0000e+00\n",
      "Epoch 682/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 18.4731 - accuracy: 0.0000e+00\n",
      "Epoch 683/1000\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 18.4255 - accuracy: 0.0000e+00\n",
      "Epoch 684/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 18.3757 - accuracy: 0.0000e+00\n",
      "Epoch 685/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 18.3254 - accuracy: 0.0000e+00\n",
      "Epoch 686/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 18.2760 - accuracy: 0.0000e+00\n",
      "Epoch 687/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 18.2287 - accuracy: 0.0000e+00\n",
      "Epoch 688/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 18.1840 - accuracy: 0.0000e+00\n",
      "Epoch 689/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 18.1418 - accuracy: 0.0000e+00\n",
      "Epoch 690/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 18.1071 - accuracy: 0.0000e+00\n",
      "Epoch 691/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 18.0590 - accuracy: 0.0000e+00\n",
      "Epoch 692/1000\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 18.0185 - accuracy: 0.0000e+00\n",
      "Epoch 693/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 17.9794 - accuracy: 0.0000e+00\n",
      "Epoch 694/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 17.9411 - accuracy: 0.0000e+00\n",
      "Epoch 695/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 17.9029 - accuracy: 0.0000e+00\n",
      "Epoch 696/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 17.8641 - accuracy: 0.0000e+00\n",
      "Epoch 697/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 17.8247 - accuracy: 0.0000e+00\n",
      "Epoch 698/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 17.7845 - accuracy: 0.0000e+00\n",
      "Epoch 699/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 17.7437 - accuracy: 0.0000e+00\n",
      "Epoch 700/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 17.7027 - accuracy: 0.0000e+00\n",
      "Epoch 701/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 17.6618 - accuracy: 0.0000e+00\n",
      "Epoch 702/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 17.6213 - accuracy: 0.0000e+00\n",
      "Epoch 703/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 17.5815 - accuracy: 0.0000e+00\n",
      "Epoch 704/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 17.5424 - accuracy: 0.0000e+00\n",
      "Epoch 705/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 17.5041 - accuracy: 0.0000e+00\n",
      "Epoch 706/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 17.4662 - accuracy: 0.0000e+00\n",
      "Epoch 707/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 17.4287 - accuracy: 0.0000e+00\n",
      "Epoch 708/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 17.3913 - accuracy: 0.0000e+00\n",
      "Epoch 709/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 17.3539 - accuracy: 0.0000e+00\n",
      "Epoch 710/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 17.3165 - accuracy: 0.0000e+00\n",
      "Epoch 711/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 17.2790 - accuracy: 0.0000e+00\n",
      "Epoch 712/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 17.2417 - accuracy: 0.0000e+00\n",
      "Epoch 713/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 17.2045 - accuracy: 0.0000e+00\n",
      "Epoch 714/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 17.1675 - accuracy: 0.0000e+00\n",
      "Epoch 715/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 17.1309 - accuracy: 0.0000e+00\n",
      "Epoch 716/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 17.0947 - accuracy: 0.0000e+00\n",
      "Epoch 717/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 17.0587 - accuracy: 0.0000e+00\n",
      "Epoch 718/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 17.0231 - accuracy: 0.0000e+00\n",
      "Epoch 719/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 16.9877 - accuracy: 0.0000e+00\n",
      "Epoch 720/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 16.9526 - accuracy: 0.0000e+00\n",
      "Epoch 721/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 16.9177 - accuracy: 0.0000e+00\n",
      "Epoch 722/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 16.8829 - accuracy: 0.0000e+00\n",
      "Epoch 723/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 16.8482 - accuracy: 0.0000e+00\n",
      "Epoch 724/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 16.8139 - accuracy: 0.0000e+00\n",
      "Epoch 725/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 16.7798 - accuracy: 0.0000e+00\n",
      "Epoch 726/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 16.7458 - accuracy: 0.0000e+00\n",
      "Epoch 727/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 16.7122 - accuracy: 0.0000e+00\n",
      "Epoch 728/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 16.6788 - accuracy: 0.0000e+00\n",
      "Epoch 729/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 16.6456 - accuracy: 0.0000e+00\n",
      "Epoch 730/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 16.6128 - accuracy: 0.0000e+00\n",
      "Epoch 731/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 16.5801 - accuracy: 0.0000e+00\n",
      "Epoch 732/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 16.5477 - accuracy: 0.0000e+00\n",
      "Epoch 733/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 16.5155 - accuracy: 0.0000e+00\n",
      "Epoch 734/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 16.4836 - accuracy: 0.0000e+00\n",
      "Epoch 735/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 16.4519 - accuracy: 0.0000e+00\n",
      "Epoch 736/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 16.4226 - accuracy: 0.0000e+00\n",
      "Epoch 737/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 16.3924 - accuracy: 0.0000e+00\n",
      "Epoch 738/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 16.3692 - accuracy: 0.0000e+00\n",
      "Epoch 739/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 16.3476 - accuracy: 0.0000e+00\n",
      "Epoch 740/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 16.3250 - accuracy: 0.0000e+00\n",
      "Epoch 741/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 16.2992 - accuracy: 0.0000e+00\n",
      "Epoch 742/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 16.2697 - accuracy: 0.0000e+00\n",
      "Epoch 743/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 16.2368 - accuracy: 0.0000e+00\n",
      "Epoch 744/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 16.2017 - accuracy: 0.0000e+00\n",
      "Epoch 745/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 16.1661 - accuracy: 0.0000e+00\n",
      "Epoch 746/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 16.1315 - accuracy: 0.0000e+00\n",
      "Epoch 747/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 16.0990 - accuracy: 0.0000e+00\n",
      "Epoch 748/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 16.0692 - accuracy: 0.0000e+00\n",
      "Epoch 749/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 16.0418 - accuracy: 0.0000e+00\n",
      "Epoch 750/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 16.0159 - accuracy: 0.0000e+00\n",
      "Epoch 751/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 16.0249 - accuracy: 0.0000e+00\n",
      "Epoch 752/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 15.9605 - accuracy: 0.0000e+00\n",
      "Epoch 753/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 15.9333 - accuracy: 0.0000e+00\n",
      "Epoch 754/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 15.9085 - accuracy: 0.0000e+00\n",
      "Epoch 755/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 15.8855 - accuracy: 0.0000e+00\n",
      "Epoch 756/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 15.8628 - accuracy: 0.0000e+00\n",
      "Epoch 757/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 15.8395 - accuracy: 0.0000e+00\n",
      "Epoch 758/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 15.8150 - accuracy: 0.0000e+00\n",
      "Epoch 759/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 15.7892 - accuracy: 0.0000e+00\n",
      "Epoch 760/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 15.7624 - accuracy: 0.0000e+00\n",
      "Epoch 761/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 15.7351 - accuracy: 0.0000e+00\n",
      "Epoch 762/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 15.7079 - accuracy: 0.0000e+00\n",
      "Epoch 763/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 15.6813 - accuracy: 0.0000e+00\n",
      "Epoch 764/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 15.6557 - accuracy: 0.0000e+00\n",
      "Epoch 765/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 15.6314 - accuracy: 0.0000e+00\n",
      "Epoch 766/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 15.6079 - accuracy: 0.0000e+00\n",
      "Epoch 767/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 15.5852 - accuracy: 0.0000e+00\n",
      "Epoch 768/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 15.5628 - accuracy: 0.0000e+00\n",
      "Epoch 769/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 15.5404 - accuracy: 0.0000e+00\n",
      "Epoch 770/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 15.5179 - accuracy: 0.0000e+00\n",
      "Epoch 771/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 15.4951 - accuracy: 0.0000e+00\n",
      "Epoch 772/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 15.4722 - accuracy: 0.0000e+00\n",
      "Epoch 773/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 15.4494 - accuracy: 0.0000e+00\n",
      "Epoch 774/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 15.4268 - accuracy: 0.0000e+00\n",
      "Epoch 775/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 15.4046 - accuracy: 0.0000e+00\n",
      "Epoch 776/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 15.3827 - accuracy: 0.0000e+00\n",
      "Epoch 777/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 15.3614 - accuracy: 0.0000e+00\n",
      "Epoch 778/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 15.3404 - accuracy: 0.0000e+00\n",
      "Epoch 779/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 15.3198 - accuracy: 0.0000e+00\n",
      "Epoch 780/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 15.2994 - accuracy: 0.0000e+00\n",
      "Epoch 781/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 15.2791 - accuracy: 0.0000e+00\n",
      "Epoch 782/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 15.2590 - accuracy: 0.0000e+00\n",
      "Epoch 783/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 15.2389 - accuracy: 0.0000e+00\n",
      "Epoch 784/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 15.2190 - accuracy: 0.0000e+00\n",
      "Epoch 785/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 15.1992 - accuracy: 0.0000e+00\n",
      "Epoch 786/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 15.1797 - accuracy: 0.0000e+00\n",
      "Epoch 787/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 15.1604 - accuracy: 0.0000e+00\n",
      "Epoch 788/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 15.1413 - accuracy: 0.0000e+00\n",
      "Epoch 789/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 15.1226 - accuracy: 0.0000e+00\n",
      "Epoch 790/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 15.1041 - accuracy: 0.0000e+00\n",
      "Epoch 791/1000\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 15.0859 - accuracy: 0.0000e+00\n",
      "Epoch 792/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 15.0678 - accuracy: 0.0000e+00\n",
      "Epoch 793/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 15.0500 - accuracy: 0.0000e+00\n",
      "Epoch 794/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 15.0323 - accuracy: 0.0000e+00\n",
      "Epoch 795/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 15.0148 - accuracy: 0.0000e+00\n",
      "Epoch 796/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.9974 - accuracy: 0.0000e+00\n",
      "Epoch 797/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.9802 - accuracy: 0.0000e+00\n",
      "Epoch 798/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 14.9633 - accuracy: 0.0000e+00\n",
      "Epoch 799/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.9465 - accuracy: 0.0000e+00\n",
      "Epoch 800/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 14.9300 - accuracy: 0.0000e+00\n",
      "Epoch 801/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 14.9137 - accuracy: 0.0000e+00\n",
      "Epoch 802/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 14.8990 - accuracy: 0.0000e+00\n",
      "Epoch 803/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 14.8860 - accuracy: 0.0000e+00\n",
      "Epoch 804/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 14.8787 - accuracy: 0.0000e+00\n",
      "Epoch 805/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 14.8723 - accuracy: 0.0000e+00\n",
      "Epoch 806/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.8639 - accuracy: 0.0000e+00\n",
      "Epoch 807/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.8515 - accuracy: 0.0000e+00\n",
      "Epoch 808/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.8351 - accuracy: 0.0000e+00\n",
      "Epoch 809/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 14.8153 - accuracy: 0.0000e+00\n",
      "Epoch 810/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 14.7940 - accuracy: 0.0000e+00\n",
      "Epoch 811/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 14.7730 - accuracy: 0.0000e+00\n",
      "Epoch 812/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 14.7539 - accuracy: 0.0000e+00\n",
      "Epoch 813/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.7373 - accuracy: 0.0000e+00\n",
      "Epoch 814/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 14.7234 - accuracy: 0.0000e+00\n",
      "Epoch 815/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.7176 - accuracy: 0.0000e+00\n",
      "Epoch 816/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 14.6970 - accuracy: 0.0000e+00\n",
      "Epoch 817/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.6842 - accuracy: 0.0000e+00\n",
      "Epoch 818/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 14.6728 - accuracy: 0.0000e+00\n",
      "Epoch 819/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 14.6620 - accuracy: 0.0000e+00\n",
      "Epoch 820/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 14.6512 - accuracy: 0.0000e+00\n",
      "Epoch 821/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.6399 - accuracy: 0.0000e+00\n",
      "Epoch 822/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 14.6278 - accuracy: 0.0000e+00\n",
      "Epoch 823/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 14.6150 - accuracy: 0.0000e+00\n",
      "Epoch 824/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 14.6018 - accuracy: 0.0000e+00\n",
      "Epoch 825/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.5884 - accuracy: 0.0000e+00\n",
      "Epoch 826/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 14.5754 - accuracy: 0.0000e+00\n",
      "Epoch 827/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.5629 - accuracy: 0.0000e+00\n",
      "Epoch 828/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.5511 - accuracy: 0.0000e+00\n",
      "Epoch 829/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.5399 - accuracy: 0.0000e+00\n",
      "Epoch 830/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.5291 - accuracy: 0.0000e+00\n",
      "Epoch 831/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 14.5185 - accuracy: 0.0000e+00\n",
      "Epoch 832/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 14.5080 - accuracy: 0.0000e+00\n",
      "Epoch 833/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.4974 - accuracy: 0.0000e+00\n",
      "Epoch 834/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 14.4866 - accuracy: 0.0000e+00\n",
      "Epoch 835/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 14.4757 - accuracy: 0.0000e+00\n",
      "Epoch 836/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 14.4648 - accuracy: 0.0000e+00\n",
      "Epoch 837/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 14.4541 - accuracy: 0.0000e+00\n",
      "Epoch 838/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 14.4435 - accuracy: 0.0000e+00\n",
      "Epoch 839/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 14.4332 - accuracy: 0.0000e+00\n",
      "Epoch 840/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 14.4232 - accuracy: 0.0000e+00\n",
      "Epoch 841/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 14.4134 - accuracy: 0.0000e+00\n",
      "Epoch 842/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 14.4038 - accuracy: 0.0000e+00\n",
      "Epoch 843/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 14.3944 - accuracy: 0.0000e+00\n",
      "Epoch 844/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 14.3850 - accuracy: 0.0000e+00\n",
      "Epoch 845/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 14.3756 - accuracy: 0.0000e+00\n",
      "Epoch 846/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 14.3663 - accuracy: 0.0000e+00\n",
      "Epoch 847/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 14.3570 - accuracy: 0.0000e+00\n",
      "Epoch 848/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 14.3479 - accuracy: 0.0000e+00\n",
      "Epoch 849/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.3388 - accuracy: 0.0000e+00\n",
      "Epoch 850/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 14.3299 - accuracy: 0.0000e+00\n",
      "Epoch 851/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 14.3212 - accuracy: 0.0000e+00\n",
      "Epoch 852/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.3126 - accuracy: 0.0000e+00\n",
      "Epoch 853/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 14.3041 - accuracy: 0.0000e+00\n",
      "Epoch 854/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.2958 - accuracy: 0.0000e+00\n",
      "Epoch 855/1000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 14.2875 - accuracy: 0.0000e+00\n",
      "Epoch 856/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 14.2794 - accuracy: 0.0000e+00\n",
      "Epoch 857/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 14.2713 - accuracy: 0.0000e+00\n",
      "Epoch 858/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.2633 - accuracy: 0.0000e+00\n",
      "Epoch 859/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 14.2553 - accuracy: 0.0000e+00\n",
      "Epoch 860/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 14.2475 - accuracy: 0.0000e+00\n",
      "Epoch 861/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 14.2398 - accuracy: 0.0000e+00\n",
      "Epoch 862/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 14.2321 - accuracy: 0.0000e+00\n",
      "Epoch 863/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 14.2246 - accuracy: 0.0000e+00\n",
      "Epoch 864/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 14.2172 - accuracy: 0.0000e+00\n",
      "Epoch 865/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 14.2099 - accuracy: 0.0000e+00\n",
      "Epoch 866/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 14.2027 - accuracy: 0.0000e+00\n",
      "Epoch 867/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 14.1955 - accuracy: 0.0000e+00\n",
      "Epoch 868/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.1884 - accuracy: 0.0000e+00\n",
      "Epoch 869/1000\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 14.1814 - accuracy: 0.0000e+00\n",
      "Epoch 870/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 14.1745 - accuracy: 0.0000e+00\n",
      "Epoch 871/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 14.1677 - accuracy: 0.0000e+00\n",
      "Epoch 872/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 14.1609 - accuracy: 0.0000e+00\n",
      "Epoch 873/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 14.1542 - accuracy: 0.0000e+00\n",
      "Epoch 874/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.1476 - accuracy: 0.0000e+00\n",
      "Epoch 875/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.1411 - accuracy: 0.0000e+00\n",
      "Epoch 876/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.1371 - accuracy: 0.0000e+00\n",
      "Epoch 877/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 14.1313 - accuracy: 0.0000e+00\n",
      "Epoch 878/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.1319 - accuracy: 0.0000e+00\n",
      "Epoch 879/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.1333 - accuracy: 0.0000e+00\n",
      "Epoch 880/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 14.1329 - accuracy: 0.0000e+00\n",
      "Epoch 881/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 14.1289 - accuracy: 0.0000e+00\n",
      "Epoch 882/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.1212 - accuracy: 0.0000e+00\n",
      "Epoch 883/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 14.1107 - accuracy: 0.0000e+00\n",
      "Epoch 884/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 14.0989 - accuracy: 0.0000e+00\n",
      "Epoch 885/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 14.0875 - accuracy: 0.0000e+00\n",
      "Epoch 886/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 14.0780 - accuracy: 0.0000e+00\n",
      "Epoch 887/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 14.0708 - accuracy: 0.0000e+00\n",
      "Epoch 888/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 14.0658 - accuracy: 0.0000e+00\n",
      "Epoch 889/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.0623 - accuracy: 0.0000e+00\n",
      "Epoch 890/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 14.0756 - accuracy: 0.0000e+00\n",
      "Epoch 891/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 14.0501 - accuracy: 0.0000e+00\n",
      "Epoch 892/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 14.0442 - accuracy: 0.0000e+00\n",
      "Epoch 893/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 14.0409 - accuracy: 0.0000e+00\n",
      "Epoch 894/1000\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 14.0389 - accuracy: 0.0000e+00\n",
      "Epoch 895/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 14.0370 - accuracy: 0.0000e+00\n",
      "Epoch 896/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 14.0340 - accuracy: 0.0000e+00\n",
      "Epoch 897/1000\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 14.0294 - accuracy: 0.0000e+00\n",
      "Epoch 898/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 14.0232 - accuracy: 0.0000e+00\n",
      "Epoch 899/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 14.0160 - accuracy: 0.0000e+00\n",
      "Epoch 900/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 14.0084 - accuracy: 0.0000e+00\n",
      "Epoch 901/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 14.0013 - accuracy: 0.0000e+00\n",
      "Epoch 902/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 13.9950 - accuracy: 0.0000e+00\n",
      "Epoch 903/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.9897 - accuracy: 0.0000e+00\n",
      "Epoch 904/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 13.9853 - accuracy: 0.0000e+00\n",
      "Epoch 905/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 13.9815 - accuracy: 0.0000e+00\n",
      "Epoch 906/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 13.9778 - accuracy: 0.0000e+00\n",
      "Epoch 907/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 13.9738 - accuracy: 0.0000e+00\n",
      "Epoch 908/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 13.9693 - accuracy: 0.0000e+00\n",
      "Epoch 909/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.9643 - accuracy: 0.0000e+00\n",
      "Epoch 910/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.9591 - accuracy: 0.0000e+00\n",
      "Epoch 911/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 13.9536 - accuracy: 0.0000e+00\n",
      "Epoch 912/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.9484 - accuracy: 0.0000e+00\n",
      "Epoch 913/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 13.9433 - accuracy: 0.0000e+00\n",
      "Epoch 914/1000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 13.9387 - accuracy: 0.0000e+00\n",
      "Epoch 915/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 13.9343 - accuracy: 0.0000e+00\n",
      "Epoch 916/1000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 13.9302 - accuracy: 0.0000e+00\n",
      "Epoch 917/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.9261 - accuracy: 0.0000e+00\n",
      "Epoch 918/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.9220 - accuracy: 0.0000e+00\n",
      "Epoch 919/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.9177 - accuracy: 0.0000e+00\n",
      "Epoch 920/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 13.9133 - accuracy: 0.0000e+00\n",
      "Epoch 921/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.9088 - accuracy: 0.0000e+00\n",
      "Epoch 922/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.9043 - accuracy: 0.0000e+00\n",
      "Epoch 923/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.8998 - accuracy: 0.0000e+00\n",
      "Epoch 924/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 13.8954 - accuracy: 0.0000e+00\n",
      "Epoch 925/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.8912 - accuracy: 0.0000e+00\n",
      "Epoch 926/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 13.8870 - accuracy: 0.0000e+00\n",
      "Epoch 927/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 13.8830 - accuracy: 0.0000e+00\n",
      "Epoch 928/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 13.8790 - accuracy: 0.0000e+00\n",
      "Epoch 929/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 13.8749 - accuracy: 0.0000e+00\n",
      "Epoch 930/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 13.8709 - accuracy: 0.0000e+00\n",
      "Epoch 931/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.8668 - accuracy: 0.0000e+00\n",
      "Epoch 932/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.8627 - accuracy: 0.0000e+00\n",
      "Epoch 933/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.8586 - accuracy: 0.0000e+00\n",
      "Epoch 934/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.8546 - accuracy: 0.0000e+00\n",
      "Epoch 935/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 13.8505 - accuracy: 0.0000e+00\n",
      "Epoch 936/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 13.8466 - accuracy: 0.0000e+00\n",
      "Epoch 937/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 13.8426 - accuracy: 0.0000e+00\n",
      "Epoch 938/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 13.8387 - accuracy: 0.0000e+00\n",
      "Epoch 939/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 13.8349 - accuracy: 0.0000e+00\n",
      "Epoch 940/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 13.8310 - accuracy: 0.0000e+00\n",
      "Epoch 941/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.8272 - accuracy: 0.0000e+00\n",
      "Epoch 942/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.8233 - accuracy: 0.0000e+00\n",
      "Epoch 943/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.8194 - accuracy: 0.0000e+00\n",
      "Epoch 944/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.8156 - accuracy: 0.0000e+00\n",
      "Epoch 945/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 13.8117 - accuracy: 0.0000e+00\n",
      "Epoch 946/1000\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 13.8080 - accuracy: 0.0000e+00\n",
      "Epoch 947/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 13.8042 - accuracy: 0.0000e+00\n",
      "Epoch 948/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 13.8004 - accuracy: 0.0000e+00\n",
      "Epoch 949/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.7967 - accuracy: 0.0000e+00\n",
      "Epoch 950/1000\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 13.7929 - accuracy: 0.0000e+00\n",
      "Epoch 951/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 13.7892 - accuracy: 0.0000e+00\n",
      "Epoch 952/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 13.7855 - accuracy: 0.0000e+00\n",
      "Epoch 953/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.7818 - accuracy: 0.0000e+00\n",
      "Epoch 954/1000\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 13.7781 - accuracy: 0.0000e+00\n",
      "Epoch 955/1000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 13.7744 - accuracy: 0.0000e+00\n",
      "Epoch 956/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.7707 - accuracy: 0.0000e+00\n",
      "Epoch 957/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.7671 - accuracy: 0.0000e+00\n",
      "Epoch 958/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.7634 - accuracy: 0.0000e+00\n",
      "Epoch 959/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.7598 - accuracy: 0.0000e+00\n",
      "Epoch 960/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 13.7561 - accuracy: 0.0000e+00\n",
      "Epoch 961/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 13.7525 - accuracy: 0.0000e+00\n",
      "Epoch 962/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.7489 - accuracy: 0.0000e+00\n",
      "Epoch 963/1000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 13.7453 - accuracy: 0.0000e+00\n",
      "Epoch 964/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 13.7417 - accuracy: 0.0000e+00\n",
      "Epoch 965/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.7381 - accuracy: 0.0000e+00\n",
      "Epoch 966/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 13.7345 - accuracy: 0.0000e+00\n",
      "Epoch 967/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 13.7309 - accuracy: 0.0000e+00\n",
      "Epoch 968/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.7273 - accuracy: 0.0000e+00\n",
      "Epoch 969/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.7238 - accuracy: 0.0000e+00\n",
      "Epoch 970/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.7202 - accuracy: 0.0000e+00\n",
      "Epoch 971/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 13.7166 - accuracy: 0.0000e+00\n",
      "Epoch 972/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 13.7131 - accuracy: 0.0000e+00\n",
      "Epoch 973/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.7095 - accuracy: 0.0000e+00\n",
      "Epoch 974/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.7060 - accuracy: 0.0000e+00\n",
      "Epoch 975/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 13.7025 - accuracy: 0.0000e+00\n",
      "Epoch 976/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 13.6989 - accuracy: 0.0000e+00\n",
      "Epoch 977/1000\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 13.6954 - accuracy: 0.0000e+00\n",
      "Epoch 978/1000\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 13.6919 - accuracy: 0.0000e+00\n",
      "Epoch 979/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 13.6883 - accuracy: 0.0000e+00\n",
      "Epoch 980/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 13.6848 - accuracy: 0.0000e+00\n",
      "Epoch 981/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.6813 - accuracy: 0.0000e+00\n",
      "Epoch 982/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 13.6778 - accuracy: 0.0000e+00\n",
      "Epoch 983/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.6743 - accuracy: 0.0000e+00\n",
      "Epoch 984/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.6708 - accuracy: 0.0000e+00\n",
      "Epoch 985/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.6673 - accuracy: 0.0000e+00\n",
      "Epoch 986/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 13.6638 - accuracy: 0.0000e+00\n",
      "Epoch 987/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 13.6603 - accuracy: 0.0000e+00\n",
      "Epoch 988/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 13.6568 - accuracy: 0.0000e+00\n",
      "Epoch 989/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 13.6533 - accuracy: 0.0000e+00\n",
      "Epoch 990/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 13.6498 - accuracy: 0.0000e+00\n",
      "Epoch 991/1000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 13.6463 - accuracy: 0.0000e+00\n",
      "Epoch 992/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 13.6428 - accuracy: 0.0000e+00\n",
      "Epoch 993/1000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 13.6393 - accuracy: 0.0000e+00\n",
      "Epoch 994/1000\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 13.6358 - accuracy: 0.0000e+00\n",
      "Epoch 995/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.6323 - accuracy: 0.0000e+00\n",
      "Epoch 996/1000\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 13.6288 - accuracy: 0.0000e+00\n",
      "Epoch 997/1000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 13.6253 - accuracy: 0.0000e+00\n",
      "Epoch 998/1000\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 13.6222 - accuracy: 0.0000e+00\n",
      "Epoch 999/1000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 13.6218 - accuracy: 0.0000e+00\n",
      "Epoch 1000/1000\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 13.6258 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model_temp.compile(optimizer = tf.keras.optimizers.Adam(), loss='mse', metrics=['accuracy'])\n",
    "history = model_temp.fit(\n",
    "    x, y, epochs=1000, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 160ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4492.485476610354\n"
     ]
    }
   ],
   "source": [
    "y_result = model_temp.predict(x)\n",
    "plt.plot(x, y, '.')\n",
    "plt.plot(x, y_result, '-')\n",
    "plt.show()\n",
    "\n",
    "print(\"Loss:\", np.sum((y - y_result)**2)/len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
